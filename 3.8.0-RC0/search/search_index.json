{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hopsworks Model Management","text":"<p>HSML is the library to interact with the Hopsworks Model Registry and Model Serving. The library makes it easy to export, manage and deploy models.</p> <p>However, to connect from an external Python environment additional connection information, such as host and port, is required.</p>"},{"location":"#getting-started-on-hopsworks","title":"Getting Started On Hopsworks","text":"<p>Get started easily by registering an account on Hopsworks Serverless. Create your project and a new Api key. In a new python environment with Python 3.8 or higher, install the client library using pip:</p> <pre><code># Get all Hopsworks SDKs: Feature Store, Model Serving and Platform SDK\npip install hopsworks\n# or just the Model Registry and Model Serving SDK\npip install hsml\n</code></pre> <p>You can start a notebook and instantiate a connection and get the project feature store handler.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login() # you will be prompted for your api key\n\nmr = project.get_model_registry()\n# or\nms = project.get_model_serving()\n</code></pre> <p>or using <code>hsml</code> directly:</p> <pre><code>import hsml\n\nconnection = hsml.connection(\n    host=\"c.app.hopsworks.ai\", #\n    project=\"your-project\",\n    api_key_value=\"your-api-key\",\n)\n\nmr = connection.get_model_registry()\n# or\nms = connection.get_model_serving()\n</code></pre> <p>Create a new model <pre><code>model = mr.tensorflow.create_model(name=\"mnist\",\n                                   version=1,\n                                   metrics={\"accuracy\": 0.94},\n                                   description=\"mnist model description\")\nmodel.save(\"/tmp/model_directory\") # or /tmp/model_file\n</code></pre></p> <p>Download a model <pre><code>model = mr.get_model(\"mnist\", version=1)\n\nmodel_path = model.download()\n</code></pre></p> <p>Delete a model <pre><code>model.delete()\n</code></pre></p> <p>Get best performing model <pre><code>best_model = mr.get_best_model('mnist', 'accuracy', 'max')\n</code></pre></p> <p>Deploy a model <pre><code>deployment = model.deploy()\n</code></pre></p> <p>Start a deployment <pre><code>deployment.start()\n</code></pre></p> <p>Make predictions with a deployed model <pre><code>data = { \"instances\": [ model.input_example ] }\n\npredictions = deployment.predict(data)\n</code></pre></p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>You can find more examples on how to use the library in our tutorials.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation is available at Hopsworks Model Management Documentation.</p>"},{"location":"#issues","title":"Issues","text":"<p>For general questions about the usage of Hopsworks Machine Learning please open a topic on Hopsworks Community. Please report any issue using Github issue tracking.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you would like to contribute to this library, please see the Contribution Guidelines.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#python-development-setup","title":"Python development setup","text":"<ul> <li> <p>Fork and clone the repository</p> </li> <li> <p>Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda</p> </li> <li> <p>Install repository in editable mode with development dependencies:</p> <pre><code>cd python\npip install -e \".[dev]\"\n</code></pre> </li> <li> <p>Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Model Registry uses pre-commit to ensure code-style and code formatting through ruff. Run the following commands from the <code>python</code> directory:</p> <pre><code>cd python\npip install --user pre-commit\npre-commit install\n</code></pre> </li> </ul> <p>Afterwards, pre-commit will run whenever you commit.</p> <ul> <li> <p>To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use ruff:</p> <pre><code>cd python\nruff check --fix\nruff format\n</code></pre> </li> </ul>"},{"location":"CONTRIBUTING/#python-documentation","title":"Python documentation","text":"<p>We follow a few best practices for writing the Python documentation:</p> <ol> <li> <p>Use the google docstring style:</p> <pre><code>\"\"\"[One Line Summary]\n\n[Extended Summary]\n\n[!!! example\n    import xyz\n]\n\n# Arguments\n    arg1: Type[, optional]. Description[, defaults to `default`]\n    arg2: Type[, optional]. Description[, defaults to `default`]\n\n# Returns\n    Type. Description.\n\n# Raises\n    Exception. Description.\n\"\"\"\n</code></pre> <p>If Python 3 type annotations are used, they are inserted automatically.</p> </li> <li> <p>Model registry entity engine methods (e.g. ModelEngine etc.) only require a single line docstring.</p> </li> <li>REST Api implementations (e.g. ModelApi etc.) should be fully documented with docstrings without defaults.</li> <li>Public Api such as metadata objects should be fully documented with defaults.</li> </ol>"},{"location":"CONTRIBUTING/#setup-and-build-documentation","title":"Setup and Build Documentation","text":"<p>We use <code>mkdocs</code> together with <code>mike</code> (for versioning) to build the documentation and a plugin called <code>keras-autodoc</code> to auto generate Python API documentation from docstrings.</p> <p>Background about <code>mike</code>: <code>mike</code> builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, <code>mike</code> maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like <code>dev</code> or <code>latest</code>, to indicate stable and unstable releases.</p> <ol> <li> <p>Currently we are using our own version of <code>keras-autodoc</code></p> <pre><code>pip install git+https://github.com/logicalclocks/keras-autodoc\n</code></pre> </li> <li> <p>Install HSML with <code>docs</code> extras:</p> <pre><code>pip install -e .[dev,docs]\n</code></pre> </li> <li> <p>To build the docs, first run the auto doc script:</p> <pre><code>cd ..\npython auto_doc.py\n</code></pre> </li> </ol>"},{"location":"CONTRIBUTING/#option-1-build-only-current-version-of-docs","title":"Option 1: Build only current version of docs","text":"<ol> <li> <p>Either build the docs, or serve them dynamically:</p> <p>Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. <code>docs.hopsworks.ai/[version-or-alias]</code>. Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended.</p> <pre><code>mkdocs build\n# or\nmkdocs serve\n</code></pre> </li> </ol>"},{"location":"CONTRIBUTING/#option-2-preferred-build-multi-version-doc-with-mike","title":"Option 2 (Preferred): Build multi-version doc with <code>mike</code>","text":""},{"location":"CONTRIBUTING/#versioning-on-docshopsworksai","title":"Versioning on docs.hopsworks.ai","text":"<p>On docs.hopsworks.ai we implement the following versioning scheme:</p> <ul> <li>current master branches (e.g. of hsml corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev], where <code>dev</code> is an alias to indicate that this is an unstable version.</li> <li>the latest release: rendered with full current version, e.g. 2.1.5 [latest] with <code>latest</code> alias to indicate that this is the latest stable release.</li> <li>previous stable releases: rendered without alias, e.g. 2.1.4.</li> </ul>"},{"location":"CONTRIBUTING/#build-instructions","title":"Build Instructions","text":"<ol> <li> <p>For this you can either checkout and make a local copy of the <code>upstream/gh-pages</code> branch, where <code>mike</code> maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating:</p> <p>Building one branch:</p> <p>Checkout your dev branch with modified docs: <pre><code>git checkout [dev-branch]\n</code></pre></p> <p>Generate API docs if necessary: <pre><code>python auto_doc.py\n</code></pre></p> <p>Build docs with a version and alias <pre><code>mike deploy [version] [alias] --update-alias\n\n# for example, if you are updating documentation to be merged to master,\n# which will become the new SNAPSHOT version:\nmike deploy 2.2.0-SNAPSHOT dev --update-alias\n\n# if you are updating docs of the latest stable release branch\nmike deploy [version] latest --update-alias\n\n# if you are updating docs of a previous stable release branch\nmike deploy [version]\n</code></pre></p> <p>If no gh-pages branch existed in your local repository, this will have created it.</p> <p>Important: If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows</p> <pre><code>mike set-default [version-or-alias]\n</code></pre> <p>You can now checkout the gh-pages branch and serve: <pre><code>git checkout gh-pages\nmike serve\n</code></pre></p> <p>You can also list all available versions/aliases: <pre><code>mike list\n</code></pre></p> <p>Delete and reset your local gh-pages branch: <pre><code>mike delete --all\n\n# or delete single version\nmike delete [version-or-alias]\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#adding-new-api-documentation","title":"Adding new API documentation","text":"<p>To add new documentation for APIs, you need to add information about the method/class to document to the <code>auto_doc.py</code> script:</p> <pre><code>PAGES = {\n    \"connection.md\": [\n        \"hsml.connection.Connection.connection\",\n        \"hsml.connection.Connection.setup_databricks\",\n    ]\n    \"new_template.md\": [\n            \"module\",\n            \"xyz.asd\"\n    ]\n}\n</code></pre> <p>Now you can add a template markdown file to the <code>docs/templates</code> directory with the name you specified in the auto-doc script. The <code>new_template.md</code> file should contain a tag to identify the place at which the API documentation should be inserted:</p> <pre><code>## The XYZ package\n\n{{module}}\n\nSome extra content here.\n\n!!! example\n    ```python\n    import xyz\n    ```\n\n{{xyz.asd}}\n</code></pre> <p>Finally, run the <code>auto_doc.py</code> script, as decribed above, to update the documentation.</p> <p>For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation.</p>"},{"location":"generated/connection_api/","title":"Connection","text":"<p>[source]</p>"},{"location":"generated/connection_api/#connection_1","title":"Connection","text":"<pre><code>hsml.connection.Connection(\n    host=None,\n    port=443,\n    project=None,\n    hostname_verification=True,\n    trust_store_path=None,\n    api_key_file=None,\n    api_key_value=None,\n)\n</code></pre> <p>A Hopsworks Model Management connection object.</p> <p>The connection is project specific, so you can access the project's own Model Registry and Model Serving.</p> <p>This class provides convenience classmethods accessible from the <code>hsml</code>-module:</p> <p>Connection factory</p> <p>For convenience, <code>hsml</code> provides a factory method, accessible from the top level module, so you don't have to import the <code>Connection</code> class manually:</p> <pre><code>import hsml\nconn = hsml.connection()\n</code></pre> <p>Save API Key as File</p> <p>To get started quickly, you can simply create a file with the previously  created Hopsworks API Key and place it on the environment from which you  wish to connect to Hopsworks.</p> <p>You can then connect by simply passing the path to the key file when instantiating a connection:</p> <pre><code>    import hsml\n    conn = hsml.connection(\n        'my_instance',                      # DNS of your Hopsworks instance\n        443,                                # Port to reach your Hopsworks instance, defaults to 443\n        'my_project',                       # Name of your Hopsworks project\n        api_key_file='modelregistry.key',   # The file containing the API key generated above\n        hostname_verification=True)         # Disable for self-signed certificates\n    )\n    mr = conn.get_model_registry()          # Get the project's default model registry\n    ms = conn.get_model_serving()           # Uses the previous model registry\n</code></pre> <p>Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides.</p> <p>Arguments</p> <ul> <li>host <code>str | None</code>: The hostname of the Hopsworks instance, defaults to <code>None</code>.</li> <li>port <code>int</code>: The port on which the Hopsworks instance can be reached,     defaults to <code>443</code>.</li> <li>project <code>str | None</code>: The name of the project to connect to. When running on Hopsworks, this     defaults to the project from where the client is run from.     Defaults to <code>None</code>.</li> <li>hostname_verification <code>bool</code>: Whether or not to verify Hopsworks certificate, defaults     to <code>True</code>.</li> <li>trust_store_path <code>str | None</code>: Path on the file system containing the Hopsworks certificates,     defaults to <code>None</code>.</li> <li>api_key_file <code>str | None</code>: Path to a file containing the API Key.</li> <li>api_key_value <code>str | None</code>: API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>Connection</code>. Connection handle to perform operations on a Hopsworks project.</p>"},{"location":"generated/connection_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/connection_api/#api_key_file","title":"api_key_file","text":"<p>[source]</p>"},{"location":"generated/connection_api/#api_key_value","title":"api_key_value","text":"<p>[source]</p>"},{"location":"generated/connection_api/#host","title":"host","text":"<p>[source]</p>"},{"location":"generated/connection_api/#hostname_verification","title":"hostname_verification","text":"<p>[source]</p>"},{"location":"generated/connection_api/#port","title":"port","text":"<p>[source]</p>"},{"location":"generated/connection_api/#project","title":"project","text":""},{"location":"generated/connection_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/connection_api/#close","title":"close","text":"<pre><code>Connection.close()\n</code></pre> <p>Close a connection gracefully.</p> <p>This will clean up any materialized certificates on the local file system of external environments.</p> <p>Usage is recommended but optional.</p> <p>[source]</p>"},{"location":"generated/connection_api/#connect","title":"connect","text":"<pre><code>Connection.connect()\n</code></pre> <p>Instantiate the connection.</p> <p>Creating a <code>Connection</code> object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the <code>close()</code> method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call <code>connect()</code> again to reopen the connection.</p> <p>Example</p> <pre><code>import hsml\nconn = hsml.connection()\nconn.close()\nconn.connect()\n</code></pre> <p>[source]</p>"},{"location":"generated/connection_api/#connection_2","title":"connection","text":"<pre><code>Connection.connection(\n    host=None,\n    port=443,\n    project=None,\n    hostname_verification=True,\n    trust_store_path=None,\n    api_key_file=None,\n    api_key_value=None,\n)\n</code></pre> <p>Connection factory method, accessible through <code>hsml.connection()</code>.</p> <p>[source]</p>"},{"location":"generated/connection_api/#get_model_registry","title":"get_model_registry","text":"<pre><code>Connection.get_model_registry(project=None)\n</code></pre> <p>Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the <code>project</code> argument.</p> <p>Arguments</p> <ul> <li>project <code>str</code>: The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>ModelRegistry</code>. A model registry handle object to perform operations on.</p> <p>[source]</p>"},{"location":"generated/connection_api/#get_model_serving","title":"get_model_serving","text":"<pre><code>Connection.get_model_serving()\n</code></pre> <p>Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>ModelServing</code>. A model serving handle object to perform operations on.</p>"},{"location":"generated/model-registry/links/","title":"Provenance Links","text":"<p>Provenance Links are objects returned by methods such as get_feature_view_provenance, get_training_dataset_provenance. These methods use the provenance graph to return the parent feature view/training dataset of a model. These methods will return the actual instances of the feature view/training dataset if available. If the instance was deleted, or it belongs to a featurestore that the current project doesn't have access anymore, an Artifact object is returned.</p> <p>There is an additional method using the provenance graph: get_feature_view. This method wraps the <code>get_feature_view_provenance</code> and always returns a correct, usable Feature View object or throws an exception if the returned object is an Artifact. Thus an exception is thrown if the feature view was deleted or the featurestore it belongs to was unshared.</p>"},{"location":"generated/model-registry/links/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-registry/links/#accessible","title":"accessible","text":"<p>List of [FeatureView|TrainingDataset objects] objects which are part of the provenance graph requested. These entities exist in the feature store and the user has access to them.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#deleted","title":"deleted","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (feature views, training datasets) they represent. These entities have been removed from the feature store.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#faulty","title":"faulty","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (feature views, training datasets) they represent. These entities exist in the feature store, however they are corrupted.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#inaccessible","title":"inaccessible","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (feature views, training datasets) they represent. These entities exist in the feature store, however the user does not have access to them anymore.</p>"},{"location":"generated/model-registry/links/#artifact","title":"Artifact","text":"<p>Artifacts objects are part of the provenance graph and contain a minimal set of information regarding the entities (feature views, training datasets) they represent. The provenance graph contains Artifact objects when the underlying entities have been deleted or they are corrupted or they are not accessible by the current project anymore.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#model_registry_id","title":"model_registry_id","text":"<p>Id of the model registry in which the artifact is located.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#name","title":"name","text":"<p>Name of the artifact.</p> <p>[source]</p>"},{"location":"generated/model-registry/links/#version","title":"version","text":"<p>Version of the artifact</p>"},{"location":"generated/model-registry/model_api/","title":"Model","text":""},{"location":"generated/model-registry/model_api/#creation-of-a-tensorflow-model","title":"Creation of a TensorFlow model","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#create_model","title":"create_model","text":"<pre><code>hsml.model_registry.ModelRegistry.tensorflow.create_model(\n    name,\n    version=None,\n    metrics=None,\n    description=None,\n    input_example=None,\n    model_schema=None,\n    feature_view=None,\n    training_dataset_version=None,\n)\n</code></pre> <p>Create a TensorFlow model metadata object.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the <code>save()</code> method with a local file path to the directory containing the model artifacts.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to create.</li> <li>version <code>int | None</code>: Optionally version of the model to create, defaults to <code>None</code> and     will create the model with incremented version from the last     version in the model registry.</li> <li>metrics <code>dict | None</code>: Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE)</li> <li>description <code>str | None</code>: Optionally a string describing the model, defaults to empty string     <code>\"\"</code>.</li> <li>input_example <code>pandas.core.frame.DataFrame | pandas.core.series.Series | numpy.ndarray | list | None</code>: Optionally an input example that represents a single input for the model, defaults to <code>None</code>.</li> <li>model_schema <code>hsml.model_schema.ModelSchema | None</code>: Optionally a model schema for the model inputs and/or outputs.</li> </ul> <p>Returns</p> <p><code>Model</code>. The model metadata object.</p>"},{"location":"generated/model-registry/model_api/#creation-of-a-torch-model","title":"Creation of a Torch model","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#create_model_1","title":"create_model","text":"<pre><code>hsml.model_registry.ModelRegistry.torch.create_model(\n    name,\n    version=None,\n    metrics=None,\n    description=None,\n    input_example=None,\n    model_schema=None,\n    feature_view=None,\n    training_dataset_version=None,\n)\n</code></pre> <p>Create a Torch model metadata object.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the <code>save()</code> method with a local file path to the directory containing the model artifacts.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to create.</li> <li>version <code>int | None</code>: Optionally version of the model to create, defaults to <code>None</code> and     will create the model with incremented version from the last     version in the model registry.</li> <li>metrics <code>dict | None</code>: Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE)</li> <li>description <code>str | None</code>: Optionally a string describing the model, defaults to empty string     <code>\"\"</code>.</li> <li>input_example <code>pandas.core.frame.DataFrame | pandas.core.series.Series | numpy.ndarray | list | None</code>: Optionally an input example that represents a single input for the model, defaults to <code>None</code>.</li> <li>model_schema <code>hsml.model_schema.ModelSchema | None</code>: Optionally a model schema for the model inputs and/or outputs.</li> </ul> <p>Returns</p> <p><code>Model</code>. The model metadata object.</p>"},{"location":"generated/model-registry/model_api/#creation-of-a-scikit-learn-model","title":"Creation of a scikit-learn model","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#create_model_2","title":"create_model","text":"<pre><code>hsml.model_registry.ModelRegistry.sklearn.create_model(\n    name,\n    version=None,\n    metrics=None,\n    description=None,\n    input_example=None,\n    model_schema=None,\n    feature_view=None,\n    training_dataset_version=None,\n)\n</code></pre> <p>Create an SkLearn model metadata object.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the <code>save()</code> method with a local file path to the directory containing the model artifacts.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to create.</li> <li>version <code>int | None</code>: Optionally version of the model to create, defaults to <code>None</code> and     will create the model with incremented version from the last     version in the model registry.</li> <li>metrics <code>dict | None</code>: Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE)</li> <li>description <code>str | None</code>: Optionally a string describing the model, defaults to empty string     <code>\"\"</code>.</li> <li>input_example <code>pandas.core.frame.DataFrame | pandas.core.series.Series | numpy.ndarray | list | None</code>: Optionally an input example that represents a single input for the model, defaults to <code>None</code>.</li> <li>model_schema <code>hsml.model_schema.ModelSchema | None</code>: Optionally a model schema for the model inputs and/or outputs.</li> </ul> <p>Returns</p> <p><code>Model</code>. The model metadata object.</p>"},{"location":"generated/model-registry/model_api/#creation-of-a-generic-model","title":"Creation of a generic model","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#create_model_3","title":"create_model","text":"<pre><code>hsml.model_registry.ModelRegistry.python.create_model(\n    name,\n    version=None,\n    metrics=None,\n    description=None,\n    input_example=None,\n    model_schema=None,\n    feature_view=None,\n    training_dataset_version=None,\n)\n</code></pre> <p>Create a generic Python model metadata object.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the <code>save()</code> method with a local file path to the directory containing the model artifacts.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to create.</li> <li>version <code>int | None</code>: Optionally version of the model to create, defaults to <code>None</code> and     will create the model with incremented version from the last     version in the model registry.</li> <li>metrics <code>dict | None</code>: Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE)</li> <li>description <code>str | None</code>: Optionally a string describing the model, defaults to empty string     <code>\"\"</code>.</li> <li>input_example <code>pandas.core.frame.DataFrame | pandas.core.series.Series | numpy.ndarray | list | None</code>: Optionally an input example that represents a single input for the model, defaults to <code>None</code>.</li> <li>model_schema <code>hsml.model_schema.ModelSchema | None</code>: Optionally a model schema for the model inputs and/or outputs.</li> </ul> <p>Returns</p> <p><code>Model</code>. The model metadata object.</p>"},{"location":"generated/model-registry/model_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_model","title":"get_model","text":"<pre><code>ModelRegistry.get_model(name, version=None)\n</code></pre> <p>Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to get.</li> <li>version <code>int | None</code>: Version of the model to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>Model</code>: The model metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve model from the model registry.</li> </ul>"},{"location":"generated/model-registry/model_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#created","title":"created","text":"<p>Creation date of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#creator","title":"creator","text":"<p>Creator of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#description","title":"description","text":"<p>Description of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#environment","title":"environment","text":"<p>Input example of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#experiment_id","title":"experiment_id","text":"<p>Experiment Id of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#experiment_project_name","title":"experiment_project_name","text":"<p>experiment_project_name of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#framework","title":"framework","text":"<p>framework of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#id","title":"id","text":"<p>Id of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#input_example","title":"input_example","text":"<p>input_example of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#model_path","title":"model_path","text":"<p>path of the model with version folder omitted. Resolves to /Projects/{project_name}/Models/{name}</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#model_registry_id","title":"model_registry_id","text":"<p>model_registry_id of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#model_schema","title":"model_schema","text":"<p>model schema of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#name","title":"name","text":"<p>Name of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#program","title":"program","text":"<p>Executable used to export the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#project_name","title":"project_name","text":"<p>project_name of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#shared_registry_project_name","title":"shared_registry_project_name","text":"<p>shared_registry_project_name of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#training_dataset","title":"training_dataset","text":"<p>training_dataset of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#training_metrics","title":"training_metrics","text":"<p>Training metrics of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#user","title":"user","text":"<p>user of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#version","title":"version","text":"<p>Version of the model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#version_path","title":"version_path","text":"<p>path of the model including version folder. Resolves to /Projects/{project_name}/Models/{name}/{version}</p>"},{"location":"generated/model-registry/model_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_api/#delete","title":"delete","text":"<pre><code>Model.delete()\n</code></pre> <p>Delete the model</p> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the model and deletes the model files.</p> <p>Raises</p> <p><code>RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#delete_tag","title":"delete_tag","text":"<pre><code>Model.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a model.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#deploy","title":"deploy","text":"<pre><code>Model.deploy(\n    name=None,\n    description=None,\n    artifact_version=\"CREATE\",\n    serving_tool=None,\n    script_file=None,\n    resources=None,\n    inference_logger=None,\n    inference_batcher=None,\n    transformer=None,\n    api_protocol=\"REST\",\n)\n</code></pre> <p>Deploy the model.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nmy_deployment = my_model.deploy()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: Name of the deployment.</li> <li>description <code>str | None</code>: Description of the deployment.</li> <li>artifact_version <code>str | None</code>: Version number of the model artifact to deploy, <code>CREATE</code> to create a new model artifact or <code>MODEL-ONLY</code> to reuse the shared artifact containing only the model files.</li> <li>serving_tool <code>str | None</code>: Serving tool used to deploy the model server.</li> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Predict class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the predictor.</li> <li>inference_logger <code>hsml.inference_logger.InferenceLogger | dict | None</code>: Inference logger configuration.</li> <li>inference_batcher <code>hsml.inference_batcher.InferenceBatcher | dict | None</code>: Inference batcher configuration.</li> <li>transformer <code>hsml.transformer.Transformer | dict | None</code>: Transformer to be deployed together with the predictor.</li> <li>api_protocol <code>str | None</code>: API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object of a new or existing deployment.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#download","title":"download","text":"<pre><code>Model.download()\n</code></pre> <p>Download the model files.</p> <p>Returns</p> <p><code>str</code>: Absolute path to local folder containing the model files.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_feature_view","title":"get_feature_view","text":"<pre><code>Model.get_feature_view(init=True, online=None)\n</code></pre> <p>Get the parent feature view of this model, based on explicit provenance.  Only accessible, usable feature view objects are returned. Otherwise an Exception is raised.  For more details, call the base method - get_feature_view_provenance</p> <p>Returns</p> <p><code>FeatureView</code>: Feature View Object.</p> <p>Raises</p> <p><code>Exception</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_feature_view_provenance","title":"get_feature_view_provenance","text":"<pre><code>Model.get_feature_view_provenance()\n</code></pre> <p>Get the parent feature view of this model, based on explicit provenance. This feature view can be accessible, deleted or inaccessible. For deleted and inaccessible feature views, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_tag","title":"get_tag","text":"<pre><code>Model.get_tag(name)\n</code></pre> <p>Get the tags of a model.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_tags","title":"get_tags","text":"<pre><code>Model.get_tags()\n</code></pre> <p>Retrieves all tags attached to a model.</p> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_training_dataset_provenance","title":"get_training_dataset_provenance","text":"<pre><code>Model.get_training_dataset_provenance()\n</code></pre> <p>Get the parent training dataset of this model, based on explicit provenance. This training dataset can be accessible, deleted or inaccessible. For deleted and inaccessible training datasets, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#get_url","title":"get_url","text":"<pre><code>Model.get_url()\n</code></pre> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#save","title":"save","text":"<pre><code>Model.save(\n    model_path, await_registration=480, keep_original_files=False, upload_configuration=None\n)\n</code></pre> <p>Persist this model including model files and metadata to the model registry.</p> <p>Arguments</p> <ul> <li>model_path: Local or remote (Hopsworks file system) path to the folder where the model files are located, or path to a specific model file.</li> <li>await_registration: Awaiting time for the model to be registered in Hopsworks.</li> <li>keep_original_files: If the model files are located in hopsfs, whether to move or copy those files into the Models dataset. Default is False (i.e., model files will be moved)</li> <li>upload_configuration <code>Dict[str, Any] | None</code>: When saving a model from outside Hopsworks, the model is uploaded to the model registry using the REST APIs. Each model artifact is divided into     chunks and each chunk uploaded independently. This parameter can be used to control the upload chunk size, the parallelism and the number of retries.     <code>upload_configuration</code> can contain the following keys:<ul> <li>key <code>chunk_size</code>: size of each chunk in megabytes. Default 10.</li> <li>key <code>simultaneous_uploads</code>: number of chunks to upload in parallel. Default 3.</li> <li>key <code>max_chunk_retries</code>: number of times to retry the upload of a chunk in case of failure. Default 1.</li> </ul> </li> </ul> <p>Returns</p> <p><code>Model</code>: The model metadata object.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_api/#set_tag","title":"set_tag","text":"<pre><code>Model.set_tag(name, value)\n</code></pre> <p>Attach a tag to a model.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>str | dict</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>RestAPIError</code> in case the backend fails to add the tag.</p>"},{"location":"generated/model-registry/model_registry_api/","title":"Model Registry","text":""},{"location":"generated/model-registry/model_registry_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#get_model_registry","title":"get_model_registry","text":"<pre><code>Connection.get_model_registry(project=None)\n</code></pre> <p>Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the <code>project</code> argument.</p> <p>Arguments</p> <ul> <li>project <code>str</code>: The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>ModelRegistry</code>. A model registry handle object to perform operations on.</p>"},{"location":"generated/model-registry/model_registry_api/#modules","title":"Modules","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#project_path","title":"project_path","text":"<p>Path of the project the registry is connected to.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#python","title":"python","text":"<p>Module for exporting a generic Python model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#sklearn","title":"sklearn","text":"<p>Module for exporting a sklearn model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#tensorflow","title":"tensorflow","text":"<p>Module for exporting a TensorFlow model.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#torch","title":"torch","text":"<p>Module for exporting a torch model.</p>"},{"location":"generated/model-registry/model_registry_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#model_registry_id","title":"model_registry_id","text":"<p>Id of the model registry.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#project_id","title":"project_id","text":"<p>Id of the project the registry is connected to.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#project_name","title":"project_name","text":"<p>Name of the project the registry is connected to.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#project_path_1","title":"project_path","text":"<p>Path of the project the registry is connected to.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#shared_registry_project_name","title":"shared_registry_project_name","text":"<p>Name of the project the shared model registry originates from.</p>"},{"location":"generated/model-registry/model_registry_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#get_best_model","title":"get_best_model","text":"<pre><code>ModelRegistry.get_best_model(name, metric, direction)\n</code></pre> <p>Get the best performing model entity from the model registry. Getting the best performing model from the Model Registry means specifying in addition to the name, also a metric name corresponding to one of the keys in the training_metrics dict of the model and a direction. For example to get the model version with the highest accuracy, specify metric='accuracy' and direction='max'.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to get.</li> <li>metric <code>str</code>: Name of the key in the training metrics field to compare.</li> <li>direction <code>str</code>: 'max' to get the model entity with the highest value of the set metric, or 'min' for the lowest.</li> </ul> <p>Returns</p> <p><code>Model</code>: The model metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve model from the model registry.</li> </ul> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#get_model","title":"get_model","text":"<pre><code>ModelRegistry.get_model(name, version=None)\n</code></pre> <p>Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to get.</li> <li>version <code>int | None</code>: Version of the model to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>Model</code>: The model metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve model from the model registry.</li> </ul> <p>[source]</p>"},{"location":"generated/model-registry/model_registry_api/#get_models","title":"get_models","text":"<pre><code>ModelRegistry.get_models(name)\n</code></pre> <p>Get all model entities from the model registry for a specified name. Getting all models from the Model Registry for a given name returns a list of model entities, one for each version registered under the specified model name.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the model to get.</li> </ul> <p>Returns</p> <p><code>List[Model]</code>: A list of model metadata objects.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve model versions from the model registry.</li> </ul>"},{"location":"generated/model-registry/model_schema_api/","title":"Model Schema","text":""},{"location":"generated/model-registry/model_schema_api/#creation","title":"Creation","text":"<p>To create a ModelSchema, the schema of the Model inputs and/or Model ouputs has to be defined beforehand.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_schema_api/#schema","title":"Schema","text":"<pre><code>hsml.schema.Schema(object=None, **kwargs)\n</code></pre> <p>Create a schema for a model input or output.</p> <p>Arguments</p> <ul> <li>object <code>pandas.core.frame.DataFrame | pandas.core.series.Series | hsml.schema.pyspark.sql.dataframe.DataFrame | hsml.schema.hsfs.training_dataset.TrainingDataset | numpy.ndarray | list | None</code>: The object to construct the schema from.</li> </ul> <p>Returns</p> <p><code>Schema</code>. The schema object.</p> <p>After defining the Model inputs and/or outputs schemas, a ModelSchema can be created using its class constructor.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_schema_api/#modelschema","title":"ModelSchema","text":"<pre><code>hsml.model_schema.ModelSchema(input_schema=None, output_schema=None, **kwargs)\n</code></pre> <p>Create a schema for a model.</p> <p>Arguments</p> <ul> <li>input_schema <code>hsml.schema.Schema | None</code>: Schema to describe the inputs.</li> <li>output_schema <code>hsml.schema.Schema | None</code>: Schema to describe the outputs.</li> </ul> <p>Returns</p> <p><code>ModelSchema</code>. The model schema object.</p>"},{"location":"generated/model-registry/model_schema_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-registry/model_schema_api/#model-schema_1","title":"Model Schema","text":"<p>Model schemas can be accessed from the model metadata objects.</p> <pre><code>model.model_schema\n</code></pre>"},{"location":"generated/model-registry/model_schema_api/#model-input-ouput-schemas","title":"Model Input &amp; Ouput Schemas","text":"<p>The schemas of the Model inputs and outputs can be accessed from the ModelSchema metadata objects.</p> <pre><code>model_schema.input_schema\nmodel_schema.output_schema\n</code></pre>"},{"location":"generated/model-registry/model_schema_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-registry/model_schema_api/#to_dict","title":"to_dict","text":"<pre><code>Schema.to_dict()\n</code></pre> <p>Get dict representation of the Schema.</p> <p>[source]</p>"},{"location":"generated/model-registry/model_schema_api/#to_dict_1","title":"to_dict","text":"<pre><code>ModelSchema.to_dict()\n</code></pre> <p>Get dict representation of the ModelSchema.</p>"},{"location":"generated/model-serving/deployment_api/","title":"Deployment","text":""},{"location":"generated/model-serving/deployment_api/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_model_serving","title":"get_model_serving","text":"<pre><code>Connection.get_model_serving()\n</code></pre> <p>Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>ModelServing</code>. A model serving handle object to perform operations on.</p>"},{"location":"generated/model-serving/deployment_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#create_deployment","title":"create_deployment","text":"<pre><code>ModelServing.create_deployment(predictor, name=None)\n</code></pre> <p>Create a Deployment metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre> <p>Using the model object</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nmy_deployment = my_model.deploy()\n\nmy_deployment.get_state().describe()\n</code></pre> <p>Using the Model Serving handle</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = my_predictor.deploy()\n\nmy_deployment.get_state().describe()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the <code>save()</code> method.</p> <p>Arguments</p> <ul> <li>predictor <code>hsml.predictor.Predictor</code>: predictor to be used in the deployment</li> <li>name <code>str | None</code>: name of the deployment</li> </ul> <p>Returns</p> <p><code>Deployment</code>. The model metadata object.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#deploy","title":"deploy","text":"<pre><code>Model.deploy(\n    name=None,\n    description=None,\n    artifact_version=\"CREATE\",\n    serving_tool=None,\n    script_file=None,\n    resources=None,\n    inference_logger=None,\n    inference_batcher=None,\n    transformer=None,\n    api_protocol=\"REST\",\n)\n</code></pre> <p>Deploy the model.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nmy_deployment = my_model.deploy()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: Name of the deployment.</li> <li>description <code>str | None</code>: Description of the deployment.</li> <li>artifact_version <code>str | None</code>: Version number of the model artifact to deploy, <code>CREATE</code> to create a new model artifact or <code>MODEL-ONLY</code> to reuse the shared artifact containing only the model files.</li> <li>serving_tool <code>str | None</code>: Serving tool used to deploy the model server.</li> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Predict class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the predictor.</li> <li>inference_logger <code>hsml.inference_logger.InferenceLogger | dict | None</code>: Inference logger configuration.</li> <li>inference_batcher <code>hsml.inference_batcher.InferenceBatcher | dict | None</code>: Inference batcher configuration.</li> <li>transformer <code>hsml.transformer.Transformer | dict | None</code>: Transformer to be deployed together with the predictor.</li> <li>api_protocol <code>str | None</code>: API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object of a new or existing deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#deploy_1","title":"deploy","text":"<pre><code>Predictor.deploy()\n</code></pre> <p>Create a deployment for this predictor and persists it in the Model Serving.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\nmy_deployment = my_predictor.deploy()\n\nprint(my_deployment.get_state())\n</code></pre> <p>Returns</p> <p><code>Deployment</code>. The deployment metadata object of a new or existing deployment.</p>"},{"location":"generated/model-serving/deployment_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_deployment","title":"get_deployment","text":"<pre><code>ModelServing.get_deployment(name=None)\n</code></pre> <p>Get a deployment by name from Model Serving.</p> <p>Example</p> <pre><code># login and get Hopsworks Model Serving handle using .login() and .get_model_serving()\n\n# get a deployment by name\nmy_deployment = ms.get_deployment('deployment_name')\n</code></pre> <p>Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop).</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: Name of the deployment to get.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployment from model serving.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_deployment_by_id","title":"get_deployment_by_id","text":"<pre><code>ModelServing.get_deployment_by_id(id)\n</code></pre> <p>Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop).</p> <p>Example</p> <pre><code># login and get Hopsworks Model Serving handle using .login() and .get_model_serving()\n\n# get a deployment by id\nmy_deployment = ms.get_deployment_by_id(1)\n</code></pre> <p>Arguments</p> <ul> <li>id <code>int</code>: Id of the deployment to get.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployment from model serving.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_deployments","title":"get_deployments","text":"<pre><code>ModelServing.get_deployments(model=None, status=None)\n</code></pre> <p>Get all deployments from model serving.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nlist_deployments = ms.get_deployment(my_model)\n\nfor deployment in list_deployments:\n    print(deployment.get_state())\n</code></pre> <p>Arguments</p> <ul> <li>model <code>hsml.model.Model | None</code>: Filter by model served in the deployments</li> <li>status <code>str | None</code>: Filter by status of the deployments</li> </ul> <p>Returns</p> <p><code>List[Deployment]</code>: A list of deployments.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployments from model serving.</li> </ul>"},{"location":"generated/model-serving/deployment_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#api_protocol","title":"api_protocol","text":"<p>API protocol enabled in the deployment (e.g., HTTP or GRPC).</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#artifact_path","title":"artifact_path","text":"<p>Path of the model artifact deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#artifact_version","title":"artifact_version","text":"<p>Artifact version deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#created_at","title":"created_at","text":"<p>Created at date of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#creator","title":"creator","text":"<p>Creator of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#description","title":"description","text":"<p>Description of the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#id","title":"id","text":"<p>Id of the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#inference_batcher","title":"inference_batcher","text":"<p>Configuration of the inference batcher attached to this predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#inference_logger","title":"inference_logger","text":"<p>Configuration of the inference logger attached to this predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#model_name","title":"model_name","text":"<p>Name of the model deployed by the predictor</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#model_path","title":"model_path","text":"<p>Model path deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#model_registry_id","title":"model_registry_id","text":"<p>Model Registry Id of the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#model_server","title":"model_server","text":"<p>Model server ran by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#model_version","title":"model_version","text":"<p>Model version deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#name","title":"name","text":"<p>Name of the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#predictor","title":"predictor","text":"<p>Predictor used in the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#requested_instances","title":"requested_instances","text":"<p>Total number of requested instances in the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#resources","title":"resources","text":"<p>Resource configuration for the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#script_file","title":"script_file","text":"<p>Script file used by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#serving_tool","title":"serving_tool","text":"<p>Serving tool used to run the model server.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#transformer","title":"transformer","text":"<p>Transformer configured in the predictor.</p>"},{"location":"generated/model-serving/deployment_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#delete","title":"delete","text":"<pre><code>Deployment.delete(force=False)\n</code></pre> <p>Delete the deployment</p> <p>Arguments</p> <ul> <li>force: Force the deletion of the deployment.        If the deployment is running, it will be stopped and deleted automatically.        !!! warn A call to this method does not ask for a second confirmation.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#describe","title":"describe","text":"<pre><code>Deployment.describe()\n</code></pre> <p>Print a description of the deployment</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#download_artifact","title":"download_artifact","text":"<pre><code>Deployment.download_artifact()\n</code></pre> <p>Download the model artifact served by the deployment</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_logs","title":"get_logs","text":"<pre><code>Deployment.get_logs(component=\"predictor\", tail=10)\n</code></pre> <p>Prints the deployment logs of the predictor or transformer.</p> <p>Arguments</p> <ul> <li>component: Deployment component to get the logs from (e.g., predictor or transformer)</li> <li>tail: Number of most recent lines to retrieve from the logs.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_model","title":"get_model","text":"<pre><code>Deployment.get_model()\n</code></pre> <p>Retrieve the metadata object for the model being used by this deployment</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_state","title":"get_state","text":"<pre><code>Deployment.get_state()\n</code></pre> <p>Get the current state of the deployment</p> <p>Returns</p> <p><code>PredictorState</code>. The state of the deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#get_url","title":"get_url","text":"<pre><code>Deployment.get_url()\n</code></pre> <p>Get url to the deployment in Hopsworks</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#is_created","title":"is_created","text":"<pre><code>Deployment.is_created()\n</code></pre> <p>Check whether the deployment is created.</p> <p>Returns</p> <p><code>bool</code>. Whether the deployment is created or not.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#is_running","title":"is_running","text":"<pre><code>Deployment.is_running(or_idle=True, or_updating=True)\n</code></pre> <p>Check whether the deployment is ready to handle inference requests</p> <p>Arguments</p> <ul> <li>or_idle: Whether the idle state is considered as running (default is True)</li> <li>or_updating: Whether the updating state is considered as running (default is True)</li> </ul> <p>Returns</p> <p><code>bool</code>. Whether the deployment is ready or not.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#is_stopped","title":"is_stopped","text":"<pre><code>Deployment.is_stopped(or_created=True)\n</code></pre> <p>Check whether the deployment is stopped</p> <p>Arguments</p> <ul> <li>or_created: Whether the creating and created state is considered as stopped (default is True)</li> </ul> <p>Returns</p> <p><code>bool</code>. Whether the deployment is stopped or not.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#predict","title":"predict","text":"<pre><code>Deployment.predict(data=None, inputs=None)\n</code></pre> <p>Send inference requests to the deployment.    One of data or inputs parameters must be set. If both are set, inputs will be ignored.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# retrieve deployment by name\nmy_deployment = ms.get_deployment(\"my_deployment\")\n\n# (optional) retrieve model input example\nmy_model = project.get_model_registry()                                .get_model(my_deployment.model_name, my_deployment.model_version)\n\n# make predictions using model inputs (single or batch)\npredictions = my_deployment.predict(inputs=my_model.input_example)\n\n# or using more sophisticated inference request payloads\ndata = { \"instances\": [ my_model.input_example ], \"key2\": \"value2\" }\npredictions = my_deployment.predict(data)\n</code></pre> <p>Arguments</p> <ul> <li>data <code>Dict | hsml.client.istio.utils.infer_type.InferInput | None</code>: Payload dictionary for the inference request including the model input(s)</li> <li>inputs <code>List | Dict | None</code>: Model inputs used in the inference requests</li> </ul> <p>Returns</p> <p><code>dict</code>. Inference response.</p> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#save","title":"save","text":"<pre><code>Deployment.save(await_update=60)\n</code></pre> <p>Persist this deployment including the predictor and metadata to Model Serving.</p> <p>Arguments</p> <ul> <li>await_update <code>int | None</code>: If the deployment is running, awaiting time (seconds) for the running instances to be updated.               If the running instances are not updated within this timespan, the call to this method returns while               the update in the background.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#start","title":"start","text":"<pre><code>Deployment.start(await_running=60)\n</code></pre> <p>Start the deployment</p> <p>Arguments</p> <ul> <li>await_running <code>int | None</code>: Awaiting time (seconds) for the deployment to start.                If the deployment has not started within this timespan, the call to this method returns while                it deploys in the background.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#stop","title":"stop","text":"<pre><code>Deployment.stop(await_stopped=60)\n</code></pre> <p>Stop the deployment</p> <p>Arguments</p> <ul> <li>await_stopped <code>int | None</code>: Awaiting time (seconds) for the deployment to stop.                If the deployment has not stopped within this timespan, the call to this method returns while                it stopping in the background.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/deployment_api/#to_dict","title":"to_dict","text":"<pre><code>Deployment.to_dict()\n</code></pre>"},{"location":"generated/model-serving/inference_batcher_api/","title":"Inference batcher","text":""},{"location":"generated/model-serving/inference_batcher_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#inferencebatcher","title":"InferenceBatcher","text":"<pre><code>hsml.inference_batcher.InferenceBatcher(\n    enabled=None, max_batch_size=None, max_latency=None, timeout=None, **kwargs\n)\n</code></pre> <p>Configuration of an inference batcher for a predictor.</p> <p>Arguments</p> <ul> <li>enabled <code>bool | None</code>: Whether the inference batcher is enabled or not. The default value is <code>false</code>.</li> <li>max_batch_size <code>int | None</code>: Maximum requests batch size.</li> <li>max_latency <code>int | None</code>: Maximum latency for request batching.</li> <li>timeout <code>int | None</code>: Maximum waiting time for request batching.</li> </ul> <p>Returns</p> <p><code>InferenceLogger</code>. Configuration of an inference logger.</p>"},{"location":"generated/model-serving/inference_batcher_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-serving/inference_batcher_api/#predictorinference_batcher","title":"predictor.inference_batcher","text":"<p>Inference batchers can be accessed from the predictor metadata objects.</p> <pre><code>predictor.inference_batcher\n</code></pre> <p>Predictors can be found in the deployment metadata objects (see Predictor Reference). To retrieve a deployment, see the Deployment Reference.</p>"},{"location":"generated/model-serving/inference_batcher_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#enabled","title":"enabled","text":"<p>Whether the inference batcher is enabled or not.</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#max_batch_size","title":"max_batch_size","text":"<p>Maximum requests batch size.</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#max_latency","title":"max_latency","text":"<p>Maximum latency.</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#timeout","title":"timeout","text":"<p>Maximum timeout.</p>"},{"location":"generated/model-serving/inference_batcher_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#describe","title":"describe","text":"<pre><code>InferenceBatcher.describe()\n</code></pre> <p>Print a description of the inference batcher</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_batcher_api/#to_dict","title":"to_dict","text":"<pre><code>InferenceBatcher.to_dict()\n</code></pre>"},{"location":"generated/model-serving/inference_logger_api/","title":"Inference logger","text":""},{"location":"generated/model-serving/inference_logger_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_logger_api/#inferencelogger","title":"InferenceLogger","text":"<pre><code>hsml.inference_logger.InferenceLogger(kafka_topic={}, mode=\"ALL\", **kwargs)\n</code></pre> <p>Configuration of an inference logger for a predictor.</p> <p>Arguments</p> <ul> <li>kafka_topic <code>hsml.kafka_topic.KafkaTopic | dict | None</code>: Kafka topic to send the inference logs to. By default, a new Kafka topic is configured.</li> <li>mode <code>str | None</code>: Inference logging mode. (e.g., <code>NONE</code>, <code>ALL</code>, <code>PREDICTIONS</code>, or <code>MODEL_INPUTS</code>). By default, <code>ALL</code> inference logs are sent.</li> </ul> <p>Returns</p> <p><code>InferenceLogger</code>. Configuration of an inference logger.</p>"},{"location":"generated/model-serving/inference_logger_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-serving/inference_logger_api/#predictorinference_logger","title":"predictor.inference_logger","text":"<p>Inference loggers can be accessed from the predictor metadata objects.</p> <pre><code>predictor.inference_logger\n</code></pre> <p>Predictors can be found in the deployment metadata objects (see Predictor Reference). To retrieve a deployment, see the Deployment Reference.</p>"},{"location":"generated/model-serving/inference_logger_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_logger_api/#kafka_topic","title":"kafka_topic","text":"<p>Kafka topic to send the inference logs to.</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_logger_api/#mode","title":"mode","text":"<p>Inference logging mode (\"NONE\", \"ALL\", \"PREDICTIONS\", or \"MODEL_INPUTS\").</p>"},{"location":"generated/model-serving/inference_logger_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/inference_logger_api/#describe","title":"describe","text":"<pre><code>InferenceLogger.describe()\n</code></pre> <p>Print a description of the inference logger</p> <p>[source]</p>"},{"location":"generated/model-serving/inference_logger_api/#to_dict","title":"to_dict","text":"<pre><code>InferenceLogger.to_dict()\n</code></pre>"},{"location":"generated/model-serving/model_serving_api/","title":"Model Serving","text":""},{"location":"generated/model-serving/model_serving_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#get_model_serving","title":"get_model_serving","text":"<pre><code>Connection.get_model_serving()\n</code></pre> <p>Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>ModelServing</code>. A model serving handle object to perform operations on.</p>"},{"location":"generated/model-serving/model_serving_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#project_id","title":"project_id","text":"<p>Id of the project in which Model Serving is located.</p> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#project_name","title":"project_name","text":"<p>Name of the project in which Model Serving is located.</p> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#project_path","title":"project_path","text":"<p>Path of the project the registry is connected to.</p>"},{"location":"generated/model-serving/model_serving_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#create_deployment","title":"create_deployment","text":"<pre><code>ModelServing.create_deployment(predictor, name=None)\n</code></pre> <p>Create a Deployment metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = ms.create_deployment(my_predictor)\nmy_deployment.save()\n</code></pre> <p>Using the model object</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nmy_deployment = my_model.deploy()\n\nmy_deployment.get_state().describe()\n</code></pre> <p>Using the Model Serving handle</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = my_predictor.deploy()\n\nmy_deployment.get_state().describe()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the <code>save()</code> method.</p> <p>Arguments</p> <ul> <li>predictor <code>hsml.predictor.Predictor</code>: predictor to be used in the deployment</li> <li>name <code>str | None</code>: name of the deployment</li> </ul> <p>Returns</p> <p><code>Deployment</code>. The model metadata object.</p> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#create_predictor","title":"create_predictor","text":"<pre><code>ModelServing.create_predictor(\n    model,\n    name=None,\n    artifact_version=\"CREATE\",\n    serving_tool=None,\n    script_file=None,\n    resources=None,\n    inference_logger=None,\n    inference_batcher=None,\n    transformer=None,\n    api_protocol=\"REST\",\n)\n</code></pre> <p>Create a Predictor metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = my_predictor.deploy()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the <code>deploy()</code> method.</p> <p>Arguments</p> <ul> <li>model <code>hsml.model.Model</code>: Model to be deployed.</li> <li>name <code>str | None</code>: Name of the predictor.</li> <li>artifact_version <code>str | None</code>: Version number of the model artifact to deploy, <code>CREATE</code> to create a new model artifact or <code>MODEL-ONLY</code> to reuse the shared artifact containing only the model files.</li> <li>serving_tool <code>str | None</code>: Serving tool used to deploy the model server.</li> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Predict class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the predictor.</li> <li>inference_logger <code>hsml.inference_logger.InferenceLogger | dict | str | None</code>: Inference logger configuration.</li> <li>inference_batcher <code>hsml.inference_batcher.InferenceBatcher | dict | None</code>: Inference batcher configuration.</li> <li>transformer <code>hsml.transformer.Transformer | dict | None</code>: Transformer to be deployed together with the predictor.</li> <li>api_protocol <code>str | None</code>: API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.</li> </ul> <p>Returns</p> <p><code>Predictor</code>. The predictor metadata object.</p> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#create_transformer","title":"create_transformer","text":"<pre><code>ModelServing.create_transformer(script_file=None, resources=None)\n</code></pre> <p>Create a Transformer metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Dataset API instance\ndataset_api = project.get_dataset_api()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# create my_transformer.py Python script\nclass Transformer(object):\n    def __init__(self):\n        ''' Initialization code goes here '''\n        pass\n\n    def preprocess(self, inputs):\n        ''' Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. '''\n        return inputs\n\n    def postprocess(self, outputs):\n        ''' Transform the predictions computed by the model before returning a response '''\n        return outputs\n\nuploaded_file_path = dataset_api.upload(\"my_transformer.py\", \"Resources\", overwrite=True)\ntransformer_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n\nmy_transformer = ms.create_transformer(script_file=uploaded_file_path)\n\n# or\n\nfrom hsml.transformer import Transformer\n\nmy_transformer = Transformer(script_file)\n</code></pre> <p>Create a deployment with the transformer</p> <pre><code>my_predictor = ms.create_predictor(transformer=my_transformer)\nmy_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor, transformer=my_transformer)\nmy_deployment.save()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the <code>predictor.transformer</code> property.</p> <p>Arguments</p> <ul> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Transformer class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the transformer.</li> </ul> <p>Returns</p> <p><code>Transformer</code>. The model metadata object.</p> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#get_deployment","title":"get_deployment","text":"<pre><code>ModelServing.get_deployment(name=None)\n</code></pre> <p>Get a deployment by name from Model Serving.</p> <p>Example</p> <pre><code># login and get Hopsworks Model Serving handle using .login() and .get_model_serving()\n\n# get a deployment by name\nmy_deployment = ms.get_deployment('deployment_name')\n</code></pre> <p>Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop).</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: Name of the deployment to get.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployment from model serving.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#get_deployment_by_id","title":"get_deployment_by_id","text":"<pre><code>ModelServing.get_deployment_by_id(id)\n</code></pre> <p>Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop).</p> <p>Example</p> <pre><code># login and get Hopsworks Model Serving handle using .login() and .get_model_serving()\n\n# get a deployment by id\nmy_deployment = ms.get_deployment_by_id(1)\n</code></pre> <p>Arguments</p> <ul> <li>id <code>int</code>: Id of the deployment to get.</li> </ul> <p>Returns</p> <p><code>Deployment</code>: The deployment metadata object.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployment from model serving.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#get_deployments","title":"get_deployments","text":"<pre><code>ModelServing.get_deployments(model=None, status=None)\n</code></pre> <p>Get all deployments from model serving.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\nlist_deployments = ms.get_deployment(my_model)\n\nfor deployment in list_deployments:\n    print(deployment.get_state())\n</code></pre> <p>Arguments</p> <ul> <li>model <code>hsml.model.Model | None</code>: Filter by model served in the deployments</li> <li>status <code>str | None</code>: Filter by status of the deployments</li> </ul> <p>Returns</p> <p><code>List[Deployment]</code>: A list of deployments.</p> <p>Raises</p> <ul> <li><code>RestAPIError</code>: If unable to retrieve deployments from model serving.</li> </ul> <p>[source]</p>"},{"location":"generated/model-serving/model_serving_api/#get_inference_endpoints","title":"get_inference_endpoints","text":"<pre><code>ModelServing.get_inference_endpoints()\n</code></pre> <p>Get all inference endpoints available in the current project.</p> <p>Returns</p> <p><code>List[InferenceEndpoint]</code>: Inference endpoints for model inference</p>"},{"location":"generated/model-serving/predictor_api/","title":"Predictor","text":""},{"location":"generated/model-serving/predictor_api/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#get_model_serving","title":"get_model_serving","text":"<pre><code>Connection.get_model_serving()\n</code></pre> <p>Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>ModelServing</code>. A model serving handle object to perform operations on.</p>"},{"location":"generated/model-serving/predictor_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#create_predictor","title":"create_predictor","text":"<pre><code>ModelServing.create_predictor(\n    model,\n    name=None,\n    artifact_version=\"CREATE\",\n    serving_tool=None,\n    script_file=None,\n    resources=None,\n    inference_logger=None,\n    inference_batcher=None,\n    transformer=None,\n    api_protocol=\"REST\",\n)\n</code></pre> <p>Create a Predictor metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\n\nmy_deployment = my_predictor.deploy()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the <code>deploy()</code> method.</p> <p>Arguments</p> <ul> <li>model <code>hsml.model.Model</code>: Model to be deployed.</li> <li>name <code>str | None</code>: Name of the predictor.</li> <li>artifact_version <code>str | None</code>: Version number of the model artifact to deploy, <code>CREATE</code> to create a new model artifact or <code>MODEL-ONLY</code> to reuse the shared artifact containing only the model files.</li> <li>serving_tool <code>str | None</code>: Serving tool used to deploy the model server.</li> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Predict class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the predictor.</li> <li>inference_logger <code>hsml.inference_logger.InferenceLogger | dict | str | None</code>: Inference logger configuration.</li> <li>inference_batcher <code>hsml.inference_batcher.InferenceBatcher | dict | None</code>: Inference batcher configuration.</li> <li>transformer <code>hsml.transformer.Transformer | dict | None</code>: Transformer to be deployed together with the predictor.</li> <li>api_protocol <code>str | None</code>: API protocol to be enabled in the deployment (i.e., 'REST' or 'GRPC'). Defaults to 'REST'.</li> </ul> <p>Returns</p> <p><code>Predictor</code>. The predictor metadata object.</p>"},{"location":"generated/model-serving/predictor_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-serving/predictor_api/#deploymentpredictor","title":"deployment.predictor","text":"<p>Predictors can be accessed from the deployment metadata objects.</p> <pre><code>deployment.predictor\n</code></pre> <p>To retrieve a deployment, see the Deployment Reference.</p>"},{"location":"generated/model-serving/predictor_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#api_protocol","title":"api_protocol","text":"<p>API protocol enabled in the predictor (e.g., HTTP or GRPC).</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#artifact_path","title":"artifact_path","text":"<p>Path of the model artifact deployed by the predictor. Resolves to /Projects/{project_name}/Models/{name}/{version}/Artifacts/{artifact_version}/{name}{version}.zip</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#artifact_version","title":"artifact_version","text":"<p>Artifact version deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#created_at","title":"created_at","text":"<p>Created at date of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#creator","title":"creator","text":"<p>Creator of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#description","title":"description","text":"<p>Description of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#id","title":"id","text":"<p>Id of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#inference_batcher","title":"inference_batcher","text":"<p>Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer).</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#inference_logger","title":"inference_logger","text":"<p>Configuration of the inference logger attached to this predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#model_framework","title":"model_framework","text":"<p>Model framework of the model to be deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#model_name","title":"model_name","text":"<p>Name of the model deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#model_path","title":"model_path","text":"<p>Model path deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#model_server","title":"model_server","text":"<p>Model server used by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#model_version","title":"model_version","text":"<p>Model version deployed by the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#name","title":"name","text":"<p>Name of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#requested_instances","title":"requested_instances","text":"<p>Total number of requested instances in the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#resources","title":"resources","text":"<p>Resource configuration for the deployment component (i.e., predictor or transformer).</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#script_file","title":"script_file","text":"<p>Script file used to load and run the model.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#serving_tool","title":"serving_tool","text":"<p>Serving tool used to run the model server.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#transformer","title":"transformer","text":"<p>Transformer configuration attached to the predictor.</p>"},{"location":"generated/model-serving/predictor_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#deploy","title":"deploy","text":"<pre><code>Predictor.deploy()\n</code></pre> <p>Create a deployment for this predictor and persists it in the Model Serving.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\n# get Hopsworks Model Registry handle\nmr = project.get_model_registry()\n\n# retrieve the trained model you want to deploy\nmy_model = mr.get_model(\"my_model\", version=1)\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\nmy_predictor = ms.create_predictor(my_model)\nmy_deployment = my_predictor.deploy()\n\nprint(my_deployment.get_state())\n</code></pre> <p>Returns</p> <p><code>Deployment</code>. The deployment metadata object of a new or existing deployment.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#describe","title":"describe","text":"<pre><code>Predictor.describe()\n</code></pre> <p>Print a description of the predictor</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_api/#to_dict","title":"to_dict","text":"<pre><code>Predictor.to_dict()\n</code></pre> <p>To be implemented by the component type</p>"},{"location":"generated/model-serving/predictor_state_api/","title":"Deployment state","text":"<p>The state of a deployment corresponds to the state of the predictor configured in it.</p> <p>Note</p> <p>Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon.</p>"},{"location":"generated/model-serving/predictor_state_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#get_state","title":"get_state","text":"<pre><code>Deployment.get_state()\n</code></pre> <p>Get the current state of the deployment</p> <p>Returns</p> <p><code>PredictorState</code>. The state of the deployment.</p>"},{"location":"generated/model-serving/predictor_state_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#available_predictor_instances","title":"available_predictor_instances","text":"<p>Available predicotr instances.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#available_transformer_instances","title":"available_transformer_instances","text":"<p>Available transformer instances.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#condition","title":"condition","text":"<p>Condition of the current state of predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#deployed","title":"deployed","text":"<p>Whether the predictor is deployed or not.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#hopsworks_inference_path","title":"hopsworks_inference_path","text":"<p>Inference path in the Hopsworks REST API.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#internal_port","title":"internal_port","text":"<p>Internal port for the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#model_server_inference_path","title":"model_server_inference_path","text":"<p>Inference path in the model server</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#revision","title":"revision","text":"<p>Last revision of the predictor.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#status","title":"status","text":"<p>Status of the predictor.</p>"},{"location":"generated/model-serving/predictor_state_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#describe","title":"describe","text":"<pre><code>PredictorState.describe()\n</code></pre> <p>Print a description of the deployment state</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_api/#to_dict","title":"to_dict","text":"<pre><code>PredictorState.to_dict()\n</code></pre>"},{"location":"generated/model-serving/predictor_state_condition_api/","title":"Deployment state condition","text":"<p>The state condition of a deployment is a more detailed representation of a deployment state.</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#condition","title":"condition","text":"<p>Condition of the current state of predictor.</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#reason","title":"reason","text":"<p>Condition reason of the predictor state.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#status","title":"status","text":"<p>Condition status of the predictor state.</p> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#type","title":"type","text":"<p>Condition type of the predictor state.</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#describe","title":"describe","text":"<pre><code>PredictorStateCondition.describe()\n</code></pre> <p>[source]</p>"},{"location":"generated/model-serving/predictor_state_condition_api/#to_dict","title":"to_dict","text":"<pre><code>PredictorStateCondition.to_dict()\n</code></pre>"},{"location":"generated/model-serving/resources_api/","title":"Resources","text":""},{"location":"generated/model-serving/resources_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/resources_api/#resources_1","title":"Resources","text":"<pre><code>hsml.resources.Resources(cores, memory, gpus, **kwargs)\n</code></pre> <p>Resource configuration for a predictor or transformer.</p> <p>Arguments</p> <ul> <li>cores <code>int</code>: Number of CPUs.</li> <li>memory <code>int</code>: Memory (MB) resources.</li> <li>gpus <code>int</code>: Number of GPUs.</li> </ul> <p>Returns</p> <p><code>Resources</code>. Resource configuration for a predictor or transformer.</p>"},{"location":"generated/model-serving/resources_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-serving/resources_api/#predictorresources","title":"predictor.resources","text":"<p>Resources allocated for a preditor can be accessed from the predictor metadata object.</p> <pre><code>predictor.resources\n</code></pre> <p>Predictors can be found in the deployment metadata objects (see Predictor Reference). To retrieve a deployment, see the Deployment Reference.</p>"},{"location":"generated/model-serving/resources_api/#transformerresources","title":"transformer.resources","text":"<p>Resources allocated for a transformer can be accessed from the transformer metadata object.</p> <pre><code>transformer.resources\n</code></pre> <p>Transformer can be found in the predictor metadata objects (see Predictor Reference).</p>"},{"location":"generated/model-serving/resources_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/resources_api/#cores","title":"cores","text":"<p>Number of CPUs to be allocated per instance</p> <p>[source]</p>"},{"location":"generated/model-serving/resources_api/#gpus","title":"gpus","text":"<p>Number of GPUs to be allocated per instance</p> <p>[source]</p>"},{"location":"generated/model-serving/resources_api/#memory","title":"memory","text":"<p>Memory resources to be allocated per instance</p>"},{"location":"generated/model-serving/resources_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/resources_api/#describe","title":"describe","text":"<pre><code>Resources.describe()\n</code></pre> <p>Print a description of the resource configuration</p> <p>[source]</p>"},{"location":"generated/model-serving/resources_api/#to_dict","title":"to_dict","text":"<pre><code>Resources.to_dict()\n</code></pre>"},{"location":"generated/model-serving/transformer_api/","title":"Transformer","text":""},{"location":"generated/model-serving/transformer_api/#handle","title":"Handle","text":"<p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#get_model_serving","title":"get_model_serving","text":"<pre><code>Connection.get_model_serving()\n</code></pre> <p>Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry.</p> <p>Example</p> <pre><code>import hopsworks\n\nproject = hopsworks.login()\n\nms = project.get_model_serving()\n</code></pre> <p>Returns</p> <p><code>ModelServing</code>. A model serving handle object to perform operations on.</p>"},{"location":"generated/model-serving/transformer_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#create_transformer","title":"create_transformer","text":"<pre><code>ModelServing.create_transformer(script_file=None, resources=None)\n</code></pre> <p>Create a Transformer metadata object.</p> <p>Example</p> <pre><code># login into Hopsworks using hopsworks.login()\n\n# get Dataset API instance\ndataset_api = project.get_dataset_api()\n\n# get Hopsworks Model Serving handle\nms = project.get_model_serving()\n\n# create my_transformer.py Python script\nclass Transformer(object):\n    def __init__(self):\n        ''' Initialization code goes here '''\n        pass\n\n    def preprocess(self, inputs):\n        ''' Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. '''\n        return inputs\n\n    def postprocess(self, outputs):\n        ''' Transform the predictions computed by the model before returning a response '''\n        return outputs\n\nuploaded_file_path = dataset_api.upload(\"my_transformer.py\", \"Resources\", overwrite=True)\ntransformer_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n\nmy_transformer = ms.create_transformer(script_file=uploaded_file_path)\n\n# or\n\nfrom hsml.transformer import Transformer\n\nmy_transformer = Transformer(script_file)\n</code></pre> <p>Create a deployment with the transformer</p> <pre><code>my_predictor = ms.create_predictor(transformer=my_transformer)\nmy_deployment = my_predictor.deploy()\n\n# or\nmy_deployment = ms.create_deployment(my_predictor, transformer=my_transformer)\nmy_deployment.save()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the <code>predictor.transformer</code> property.</p> <p>Arguments</p> <ul> <li>script_file <code>str | None</code>: Path to a custom predictor script implementing the Transformer class.</li> <li>resources <code>hsml.resources.PredictorResources | dict | None</code>: Resources to be allocated for the transformer.</li> </ul> <p>Returns</p> <p><code>Transformer</code>. The model metadata object.</p>"},{"location":"generated/model-serving/transformer_api/#retrieval","title":"Retrieval","text":""},{"location":"generated/model-serving/transformer_api/#predictortransformer","title":"predictor.transformer","text":"<p>Transformers can be accessed from the predictor metadata objects.</p> <pre><code>predictor.transformer\n</code></pre> <p>Predictors can be found in the deployment metadata objects (see Predictor Reference). To retrieve a deployment, see the Deployment Reference.</p>"},{"location":"generated/model-serving/transformer_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#inference_batcher","title":"inference_batcher","text":"<p>Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer).</p> <p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#resources","title":"resources","text":"<p>Resource configuration for the deployment component (i.e., predictor or transformer).</p> <p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#script_file","title":"script_file","text":"<p>Script file ran by the deployment component (i.e., predictor or transformer).</p>"},{"location":"generated/model-serving/transformer_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#describe","title":"describe","text":"<pre><code>Transformer.describe()\n</code></pre> <p>Print a description of the transformer</p> <p>[source]</p>"},{"location":"generated/model-serving/transformer_api/#to_dict","title":"to_dict","text":"<pre><code>Transformer.to_dict()\n</code></pre> <p>To be implemented by the component type</p>"}]}