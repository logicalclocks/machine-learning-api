{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hopsworks Model Management # HSML is the library to interact with the Hopsworks Model Registry and Model Serving. The library makes it easy to export, manage and deploy models. The library automatically configures itself based on the environment it is run. However, to connect from an external Python environment additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section. Getting Started On Hopsworks # Instantiate a connection and get the project model registry and serving handles import hsml # Create a connection connection = hsml . connection () # Get the model registry handle for the project's model registry mr = connection . get_model_registry () # Get the model serving handle for the current model registry ms = connection . get_model_serving () Create a new model model = mr . tensorflow . create_model ( name = \"mnist\" , version = 1 , metrics = { \"accuracy\" : 0.94 }, description = \"mnist model description\" ) model . save ( \"/tmp/model_directory\" ) # or /tmp/model_file Download a model model = mr . get_model ( \"mnist\" , version = 1 ) model_path = model . download () Delete a model model . delete () Get best performing model best_model = mr . get_best_model ( 'mnist' , 'accuracy' , 'max' ) Deploy a model deployment = model . deploy () Start a deployment deployment . start () Make predictions with a deployed model data = { \"instances\" : [ model . input_example ] } predictions = deployment . predict ( data ) You can find more examples on how to use the library in examples.hopsworks.ai . Documentation # Documentation is available at Hopsworks Model Management Documentation . Issues # For general questions about the usage of Hopsworks Machine Learning please open a topic on Hopsworks Community . Please report any issue using Github issue tracking . Contributing # If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Hopsworks Model Management"},{"location":"#hopsworks-model-management","text":"HSML is the library to interact with the Hopsworks Model Registry and Model Serving. The library makes it easy to export, manage and deploy models. The library automatically configures itself based on the environment it is run. However, to connect from an external Python environment additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section.","title":"Hopsworks Model Management"},{"location":"#getting-started-on-hopsworks","text":"Instantiate a connection and get the project model registry and serving handles import hsml # Create a connection connection = hsml . connection () # Get the model registry handle for the project's model registry mr = connection . get_model_registry () # Get the model serving handle for the current model registry ms = connection . get_model_serving () Create a new model model = mr . tensorflow . create_model ( name = \"mnist\" , version = 1 , metrics = { \"accuracy\" : 0.94 }, description = \"mnist model description\" ) model . save ( \"/tmp/model_directory\" ) # or /tmp/model_file Download a model model = mr . get_model ( \"mnist\" , version = 1 ) model_path = model . download () Delete a model model . delete () Get best performing model best_model = mr . get_best_model ( 'mnist' , 'accuracy' , 'max' ) Deploy a model deployment = model . deploy () Start a deployment deployment . start () Make predictions with a deployed model data = { \"instances\" : [ model . input_example ] } predictions = deployment . predict ( data ) You can find more examples on how to use the library in examples.hopsworks.ai .","title":"Getting Started On Hopsworks"},{"location":"#documentation","text":"Documentation is available at Hopsworks Model Management Documentation .","title":"Documentation"},{"location":"#issues","text":"For general questions about the usage of Hopsworks Machine Learning please open a topic on Hopsworks Community . Please report any issue using Github issue tracking .","title":"Issues"},{"location":"#contributing","text":"If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Contributing"},{"location":"CONTRIBUTING/","text":"Python development setup # Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Model Registry uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsml black hsml Python documentation # We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Model registry entity engine methods (e.g. ModelEngine etc.) only require a single line docstring. REST Api implementations (e.g. ModelApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults. Setup and Build Documentation # We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/logicalclocks/keras-autodoc@split-tags-properties Install HSML with docs extras: pip install -e . [ dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py Option 1: Build only current version of docs # Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve Option 2 (Preferred): Build multi-version doc with mike # Versioning on docs.hopsworks.ai # On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsml corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 . Build Instructions # For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ] Adding new API documentation # To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsml.connection.Connection.connection\" , \"hsml.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Contributing"},{"location":"CONTRIBUTING/#python-development-setup","text":"Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Model Registry uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsml black hsml","title":"Python development setup"},{"location":"CONTRIBUTING/#python-documentation","text":"We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Model registry entity engine methods (e.g. ModelEngine etc.) only require a single line docstring. REST Api implementations (e.g. ModelApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults.","title":"Python documentation"},{"location":"CONTRIBUTING/#setup-and-build-documentation","text":"We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/logicalclocks/keras-autodoc@split-tags-properties Install HSML with docs extras: pip install -e . [ dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py","title":"Setup and Build Documentation"},{"location":"CONTRIBUTING/#option-1-build-only-current-version-of-docs","text":"Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve","title":"Option 1: Build only current version of docs"},{"location":"CONTRIBUTING/#option-2-preferred-build-multi-version-doc-with-mike","text":"","title":"Option 2 (Preferred): Build multi-version doc with mike"},{"location":"CONTRIBUTING/#versioning-on-docshopsworksai","text":"On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsml corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 .","title":"Versioning on docs.hopsworks.ai"},{"location":"CONTRIBUTING/#build-instructions","text":"For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ]","title":"Build Instructions"},{"location":"CONTRIBUTING/#adding-new-api-documentation","text":"To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsml.connection.Connection.connection\" , \"hsml.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Adding new API documentation"},{"location":"generated/connection_api/","text":"Connection # [source] Connection # hsml . connection . Connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) A Hopsworks Model Management connection object. The connection is project specific, so you can access the project's own Model Registry and Model Serving. This class provides convenience classmethods accessible from the hsml -module: Connection factory For convenience, hsml provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsml conn = hsml . connection () Save API Key as File To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks. You can then connect by simply passing the path to the key file when instantiating a connection: import hsml conn = hsml . connection ( 'my_instance' , # DNS of your Hopsworks instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks project api_key_file = 'modelregistry.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . hostname_verification bool : Whether or not to verify Hopsworks certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . api_key_file Optional[str] : Path to a file containing the API Key. api_key_value Optional[str] : API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None . Returns Connection . Connection handle to perform operations on a Hopsworks project. Properties # [source] api_key_file # [source] api_key_value # [source] host # [source] hostname_verification # [source] port # [source] project # Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call connect() again to reopen the connection. Example import hsml conn = hsml . connection () conn . close () conn . connect () [source] connection # Connection . connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsml.connection() . [source] get_model_registry # Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Example import hopsworks project = hopsworks . login () ms = project . get_model_serving () Returns ModelServing . A model serving handle object to perform operations on.","title":"Connection"},{"location":"generated/connection_api/#connection","text":"[source]","title":"Connection"},{"location":"generated/connection_api/#connection_1","text":"hsml . connection . Connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) A Hopsworks Model Management connection object. The connection is project specific, so you can access the project's own Model Registry and Model Serving. This class provides convenience classmethods accessible from the hsml -module: Connection factory For convenience, hsml provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsml conn = hsml . connection () Save API Key as File To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks. You can then connect by simply passing the path to the key file when instantiating a connection: import hsml conn = hsml . connection ( 'my_instance' , # DNS of your Hopsworks instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks project api_key_file = 'modelregistry.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . hostname_verification bool : Whether or not to verify Hopsworks certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . api_key_file Optional[str] : Path to a file containing the API Key. api_key_value Optional[str] : API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None . Returns Connection . Connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/connection_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/connection_api/#api_key_file","text":"[source]","title":"api_key_file"},{"location":"generated/connection_api/#api_key_value","text":"[source]","title":"api_key_value"},{"location":"generated/connection_api/#host","text":"[source]","title":"host"},{"location":"generated/connection_api/#hostname_verification","text":"[source]","title":"hostname_verification"},{"location":"generated/connection_api/#port","text":"[source]","title":"port"},{"location":"generated/connection_api/#project","text":"","title":"project"},{"location":"generated/connection_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/connection_api/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/connection_api/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call connect() again to reopen the connection. Example import hsml conn = hsml . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/connection_api/#connection_2","text":"Connection . connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsml.connection() . [source]","title":"connection"},{"location":"generated/connection_api/#get_model_registry","text":"Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. [source]","title":"get_model_registry"},{"location":"generated/connection_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Example import hopsworks project = hopsworks . login () ms = project . get_model_serving () Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-registry/model_api/","text":"Model # Creation of a TensorFlow model # [source] create_model # hsml . model_registry . ModelRegistry . tensorflow . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a TensorFlow model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. metrics Optional[dict] : Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE) description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents a single input for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object. Creation of a Torch model # [source] create_model # hsml . model_registry . ModelRegistry . torch . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a Torch model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. metrics Optional[dict] : Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE) description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents a single input for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object. Creation of a scikit-learn model # [source] create_model # hsml . model_registry . ModelRegistry . sklearn . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create an SkLearn model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. metrics Optional[dict] : Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE) description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents a single input for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object. Creation of a generic model # [source] create_model # hsml . model_registry . ModelRegistry . python . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a generic Python model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. metrics Optional[dict] : Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE) description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents a single input for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object. Retrieval # [source] get_model # ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. Properties # [source] created # Creation date of the model. [source] creator # Creator of the model. [source] description # Description of the model. [source] environment # Input example of the model. [source] experiment_id # Experiment Id of the model. [source] experiment_project_name # experiment_project_name of the model. [source] framework # framework of the model. [source] id # Id of the model. [source] input_example # input_example of the model. [source] model_path # path of the model with version folder omitted. Resolves to /Projects/{project_name}/Models/{name} [source] model_registry_id # model_registry_id of the model. [source] model_schema # model schema of the model. [source] name # Name of the model. [source] program # Executable used to export the model. [source] project_name # project_name of the model. [source] shared_registry_project_name # shared_registry_project_name of the model. [source] training_dataset # training_dataset of the model. [source] training_metrics # Training metrics of the model. [source] user # user of the model. [source] version # Version of the model. [source] version_path # path of the model including version folder. Resolves to /Projects/{project_name}/Models/{name}/{version} Methods # [source] delete # Model . delete () Delete the model Potentially dangerous operation This operation drops all metadata associated with this version of the model and deletes the model files. Raises RestAPIError . [source] delete_tag # Model . delete_tag ( name ) Delete a tag attached to a model. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] deploy # Model . deploy ( name = None , description = None , artifact_version = \"CREATE\" , serving_tool = None , script_file = None , resources = None , inference_logger = None , inference_batcher = None , transformer = None , ) Deploy the model. Example import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) my_deployment = my_model . deploy () Arguments name Optional[str] : Name of the deployment. description Optional[str] : Description of the deployment. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Deployment : The deployment metadata object of a new or existing deployment. [source] download # Model . download () Download the model files. Returns str : Absolute path to local folder containing the model files. [source] get_tag # Model . get_tag ( name ) Get the tags of a model. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # Model . get_tags () Retrieves all tags attached to a model. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] get_url # Model . get_url () [source] save # Model . save ( model_path , await_registration = 480 , keep_original_files = False ) Persist this model including model files and metadata to the model registry. Arguments model_path : Local or remote (Hopsworks file system) path to the folder where the model files are located, or path to a specific model file. await_registration : Awaiting time for the model to be registered in Hopsworks. keep_original_files : If the model files are located in hopsfs, whether to move or copy those files into the Models dataset. Default is False (i.e., model files will be moved) Returns Model : The model metadata object. [source] set_tag # Model . set_tag ( name , value ) Attach a tag to a model. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value Union[str, dict] : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag.","title":"Model"},{"location":"generated/model-registry/model_api/#model","text":"","title":"Model"},{"location":"generated/model-registry/model_api/#creation-of-a-tensorflow-model","text":"[source]","title":"Creation of a TensorFlow model"},{"location":"generated/model-registry/model_api/#create_model","text":"hsml . model_registry . ModelRegistry . tensorflow . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a TensorFlow model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. metrics Optional[dict] : Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE) description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents a single input for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object.","title":"create_model"},{"location":"generated/model-registry/model_api/#creation-of-a-torch-model","text":"[source]","title":"Creation of a Torch model"},{"location":"generated/model-registry/model_api/#create_model_1","text":"hsml . model_registry . ModelRegistry . torch . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a Torch model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. metrics Optional[dict] : Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE) description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents a single input for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object.","title":"create_model"},{"location":"generated/model-registry/model_api/#creation-of-a-scikit-learn-model","text":"[source]","title":"Creation of a scikit-learn model"},{"location":"generated/model-registry/model_api/#create_model_2","text":"hsml . model_registry . ModelRegistry . sklearn . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create an SkLearn model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. metrics Optional[dict] : Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE) description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents a single input for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object.","title":"create_model"},{"location":"generated/model-registry/model_api/#creation-of-a-generic-model","text":"[source]","title":"Creation of a generic model"},{"location":"generated/model-registry/model_api/#create_model_3","text":"hsml . model_registry . ModelRegistry . python . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a generic Python model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. metrics Optional[dict] : Optionally a dictionary with model evaluation metrics (e.g., accuracy, MAE) description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents a single input for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object.","title":"create_model"},{"location":"generated/model-registry/model_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-registry/model_api/#get_model","text":"ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry.","title":"get_model"},{"location":"generated/model-registry/model_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-registry/model_api/#created","text":"Creation date of the model. [source]","title":"created"},{"location":"generated/model-registry/model_api/#creator","text":"Creator of the model. [source]","title":"creator"},{"location":"generated/model-registry/model_api/#description","text":"Description of the model. [source]","title":"description"},{"location":"generated/model-registry/model_api/#environment","text":"Input example of the model. [source]","title":"environment"},{"location":"generated/model-registry/model_api/#experiment_id","text":"Experiment Id of the model. [source]","title":"experiment_id"},{"location":"generated/model-registry/model_api/#experiment_project_name","text":"experiment_project_name of the model. [source]","title":"experiment_project_name"},{"location":"generated/model-registry/model_api/#framework","text":"framework of the model. [source]","title":"framework"},{"location":"generated/model-registry/model_api/#id","text":"Id of the model. [source]","title":"id"},{"location":"generated/model-registry/model_api/#input_example","text":"input_example of the model. [source]","title":"input_example"},{"location":"generated/model-registry/model_api/#model_path","text":"path of the model with version folder omitted. Resolves to /Projects/{project_name}/Models/{name} [source]","title":"model_path"},{"location":"generated/model-registry/model_api/#model_registry_id","text":"model_registry_id of the model. [source]","title":"model_registry_id"},{"location":"generated/model-registry/model_api/#model_schema","text":"model schema of the model. [source]","title":"model_schema"},{"location":"generated/model-registry/model_api/#name","text":"Name of the model. [source]","title":"name"},{"location":"generated/model-registry/model_api/#program","text":"Executable used to export the model. [source]","title":"program"},{"location":"generated/model-registry/model_api/#project_name","text":"project_name of the model. [source]","title":"project_name"},{"location":"generated/model-registry/model_api/#shared_registry_project_name","text":"shared_registry_project_name of the model. [source]","title":"shared_registry_project_name"},{"location":"generated/model-registry/model_api/#training_dataset","text":"training_dataset of the model. [source]","title":"training_dataset"},{"location":"generated/model-registry/model_api/#training_metrics","text":"Training metrics of the model. [source]","title":"training_metrics"},{"location":"generated/model-registry/model_api/#user","text":"user of the model. [source]","title":"user"},{"location":"generated/model-registry/model_api/#version","text":"Version of the model. [source]","title":"version"},{"location":"generated/model-registry/model_api/#version_path","text":"path of the model including version folder. Resolves to /Projects/{project_name}/Models/{name}/{version}","title":"version_path"},{"location":"generated/model-registry/model_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-registry/model_api/#delete","text":"Model . delete () Delete the model Potentially dangerous operation This operation drops all metadata associated with this version of the model and deletes the model files. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/model-registry/model_api/#delete_tag","text":"Model . delete_tag ( name ) Delete a tag attached to a model. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/model-registry/model_api/#deploy","text":"Model . deploy ( name = None , description = None , artifact_version = \"CREATE\" , serving_tool = None , script_file = None , resources = None , inference_logger = None , inference_batcher = None , transformer = None , ) Deploy the model. Example import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) my_deployment = my_model . deploy () Arguments name Optional[str] : Name of the deployment. description Optional[str] : Description of the deployment. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Deployment : The deployment metadata object of a new or existing deployment. [source]","title":"deploy"},{"location":"generated/model-registry/model_api/#download","text":"Model . download () Download the model files. Returns str : Absolute path to local folder containing the model files. [source]","title":"download"},{"location":"generated/model-registry/model_api/#get_tag","text":"Model . get_tag ( name ) Get the tags of a model. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/model-registry/model_api/#get_tags","text":"Model . get_tags () Retrieves all tags attached to a model. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/model-registry/model_api/#get_url","text":"Model . get_url () [source]","title":"get_url"},{"location":"generated/model-registry/model_api/#save","text":"Model . save ( model_path , await_registration = 480 , keep_original_files = False ) Persist this model including model files and metadata to the model registry. Arguments model_path : Local or remote (Hopsworks file system) path to the folder where the model files are located, or path to a specific model file. await_registration : Awaiting time for the model to be registered in Hopsworks. keep_original_files : If the model files are located in hopsfs, whether to move or copy those files into the Models dataset. Default is False (i.e., model files will be moved) Returns Model : The model metadata object. [source]","title":"save"},{"location":"generated/model-registry/model_api/#set_tag","text":"Model . set_tag ( name , value ) Attach a tag to a model. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value Union[str, dict] : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag.","title":"set_tag"},{"location":"generated/model-registry/model_registry_api/","text":"Model Registry # Retrieval # [source] get_model_registry # Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. Modules # [source] project_path # Path of the project the registry is connected to. [source] python # Module for exporting a generic Python model. [source] sklearn # Module for exporting a sklearn model. [source] tensorflow # Module for exporting a TensorFlow model. [source] torch # Module for exporting a torch model. Properties # [source] model_registry_id # Id of the model registry. [source] project_id # Id of the project the registry is connected to. [source] project_name # Name of the project the registry is connected to. [source] project_path # Path of the project the registry is connected to. [source] shared_registry_project_name # Name of the project the shared model registry originates from. Methods # [source] get_best_model # ModelRegistry . get_best_model ( name , metric , direction ) Get the best performing model entity from the model registry. Getting the best performing model from the Model Registry means specifying in addition to the name, also a metric name corresponding to one of the keys in the training_metrics dict of the model and a direction. For example to get the model version with the highest accuracy, specify metric='accuracy' and direction='max'. Arguments name str : Name of the model to get. metric str : Name of the key in the training metrics field to compare. direction str : 'max' to get the model entity with the highest value of the set metric, or 'min' for the lowest. Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source] get_model # ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source] get_models # ModelRegistry . get_models ( name ) Get all model entities from the model registry for a specified name. Getting all models from the Model Registry for a given name returns a list of model entities, one for each version registered under the specified model name. Arguments name str : Name of the model to get. Returns List[Model] : A list of model metadata objects. Raises RestAPIError : If unable to retrieve model versions from the model registry.","title":"Model Registry"},{"location":"generated/model-registry/model_registry_api/#model-registry","text":"","title":"Model Registry"},{"location":"generated/model-registry/model_registry_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-registry/model_registry_api/#get_model_registry","text":"Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on.","title":"get_model_registry"},{"location":"generated/model-registry/model_registry_api/#modules","text":"[source]","title":"Modules"},{"location":"generated/model-registry/model_registry_api/#project_path","text":"Path of the project the registry is connected to. [source]","title":"project_path"},{"location":"generated/model-registry/model_registry_api/#python","text":"Module for exporting a generic Python model. [source]","title":"python"},{"location":"generated/model-registry/model_registry_api/#sklearn","text":"Module for exporting a sklearn model. [source]","title":"sklearn"},{"location":"generated/model-registry/model_registry_api/#tensorflow","text":"Module for exporting a TensorFlow model. [source]","title":"tensorflow"},{"location":"generated/model-registry/model_registry_api/#torch","text":"Module for exporting a torch model.","title":"torch"},{"location":"generated/model-registry/model_registry_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-registry/model_registry_api/#model_registry_id","text":"Id of the model registry. [source]","title":"model_registry_id"},{"location":"generated/model-registry/model_registry_api/#project_id","text":"Id of the project the registry is connected to. [source]","title":"project_id"},{"location":"generated/model-registry/model_registry_api/#project_name","text":"Name of the project the registry is connected to. [source]","title":"project_name"},{"location":"generated/model-registry/model_registry_api/#project_path_1","text":"Path of the project the registry is connected to. [source]","title":"project_path"},{"location":"generated/model-registry/model_registry_api/#shared_registry_project_name","text":"Name of the project the shared model registry originates from.","title":"shared_registry_project_name"},{"location":"generated/model-registry/model_registry_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-registry/model_registry_api/#get_best_model","text":"ModelRegistry . get_best_model ( name , metric , direction ) Get the best performing model entity from the model registry. Getting the best performing model from the Model Registry means specifying in addition to the name, also a metric name corresponding to one of the keys in the training_metrics dict of the model and a direction. For example to get the model version with the highest accuracy, specify metric='accuracy' and direction='max'. Arguments name str : Name of the model to get. metric str : Name of the key in the training metrics field to compare. direction str : 'max' to get the model entity with the highest value of the set metric, or 'min' for the lowest. Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source]","title":"get_best_model"},{"location":"generated/model-registry/model_registry_api/#get_model","text":"ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source]","title":"get_model"},{"location":"generated/model-registry/model_registry_api/#get_models","text":"ModelRegistry . get_models ( name ) Get all model entities from the model registry for a specified name. Getting all models from the Model Registry for a given name returns a list of model entities, one for each version registered under the specified model name. Arguments name str : Name of the model to get. Returns List[Model] : A list of model metadata objects. Raises RestAPIError : If unable to retrieve model versions from the model registry.","title":"get_models"},{"location":"generated/model-registry/model_schema_api/","text":"Model Schema # Creation # To create a ModelSchema, the schema of the Model inputs and/or Model ouputs has to be defined beforehand. [source] Schema # hsml . schema . Schema ( object = None , ** kwargs ) Create a schema for a model input or output. Arguments object Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, pyspark.sql.dataframe.DataFrame, hsfs.training_dataset.TrainingDataset, numpy.ndarray, list]] : The object to construct the schema from. Returns Schema . The schema object. After defining the Model inputs and/or outputs schemas, a ModelSchema can be created using its class constructor. [source] ModelSchema # hsml . model_schema . ModelSchema ( input_schema = None , output_schema = None , ** kwargs ) Create a schema for a model. Arguments input_schema Optional[hsml.schema.Schema] : Schema to describe the inputs. output_schema Optional[hsml.schema.Schema] : Schema to describe the outputs. Returns ModelSchema . The model schema object. Retrieval # Model Schema # Model schemas can be accessed from the model metadata objects. model . model_schema Model Input & Ouput Schemas # The schemas of the Model inputs and outputs can be accessed from the ModelSchema metadata objects. model_schema . input_schema model_schema . output_schema Methods # [source] to_dict # Schema . to_dict () Get dict representation of the Schema. [source] to_dict # ModelSchema . to_dict () Get dict representation of the ModelSchema.","title":"Model Schema"},{"location":"generated/model-registry/model_schema_api/#model-schema","text":"","title":"Model Schema"},{"location":"generated/model-registry/model_schema_api/#creation","text":"To create a ModelSchema, the schema of the Model inputs and/or Model ouputs has to be defined beforehand. [source]","title":"Creation"},{"location":"generated/model-registry/model_schema_api/#schema","text":"hsml . schema . Schema ( object = None , ** kwargs ) Create a schema for a model input or output. Arguments object Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, pyspark.sql.dataframe.DataFrame, hsfs.training_dataset.TrainingDataset, numpy.ndarray, list]] : The object to construct the schema from. Returns Schema . The schema object. After defining the Model inputs and/or outputs schemas, a ModelSchema can be created using its class constructor. [source]","title":"Schema"},{"location":"generated/model-registry/model_schema_api/#modelschema","text":"hsml . model_schema . ModelSchema ( input_schema = None , output_schema = None , ** kwargs ) Create a schema for a model. Arguments input_schema Optional[hsml.schema.Schema] : Schema to describe the inputs. output_schema Optional[hsml.schema.Schema] : Schema to describe the outputs. Returns ModelSchema . The model schema object.","title":"ModelSchema"},{"location":"generated/model-registry/model_schema_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-registry/model_schema_api/#model-schema_1","text":"Model schemas can be accessed from the model metadata objects. model . model_schema","title":"Model Schema"},{"location":"generated/model-registry/model_schema_api/#model-input-ouput-schemas","text":"The schemas of the Model inputs and outputs can be accessed from the ModelSchema metadata objects. model_schema . input_schema model_schema . output_schema","title":"Model Input &amp; Ouput Schemas"},{"location":"generated/model-registry/model_schema_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-registry/model_schema_api/#to_dict","text":"Schema . to_dict () Get dict representation of the Schema. [source]","title":"to_dict"},{"location":"generated/model-registry/model_schema_api/#to_dict_1","text":"ModelSchema . to_dict () Get dict representation of the ModelSchema.","title":"to_dict"},{"location":"generated/model-serving/deployment_api/","text":"Deployment # Handle # [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Example import hopsworks project = hopsworks . login () ms = project . get_model_serving () Returns ModelServing . A model serving handle object to perform operations on. Creation # [source] create_deployment # ModelServing . create_deployment ( predictor , name = None ) Create a Deployment metadata object. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () Using the model object # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) my_deployment = my_model . deploy () my_deployment . get_state () . describe () Using the Model Serving handle # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () my_deployment . get_state () . describe () Lazy This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the save() method. Arguments predictor hsml.predictor.Predictor : predictor to be used in the deployment name Optional[str] : name of the deployment Returns Deployment . The model metadata object. [source] deploy # Model . deploy ( name = None , description = None , artifact_version = \"CREATE\" , serving_tool = None , script_file = None , resources = None , inference_logger = None , inference_batcher = None , transformer = None , ) Deploy the model. Example import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) my_deployment = my_model . deploy () Arguments name Optional[str] : Name of the deployment. description Optional[str] : Description of the deployment. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Deployment : The deployment metadata object of a new or existing deployment. [source] deploy # Predictor . deploy () Create a deployment for this predictor and persists it in the Model Serving. Example import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () print ( my_deployment . get_state ()) Returns Deployment . The deployment metadata object of a new or existing deployment. Retrieval # [source] get_deployment # ModelServing . get_deployment ( name ) Get a deployment by name from Model Serving. Example # login and get Hopsworks Model Serving handle using .login() and .get_model_serving() # get a deployment by name my_deployment = ms . get_deployment ( 'deployment_name' ) Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments name str : Name of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source] get_deployment_by_id # ModelServing . get_deployment_by_id ( id ) Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Example # login and get Hopsworks Model Serving handle using .login() and .get_model_serving() # get a deployment by id my_deployment = ms . get_deployment_by_id ( 1 ) Arguments id int : Id of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source] get_deployments # ModelServing . get_deployments ( model = None , status = None ) Get all deployments from model serving. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) list_deployments = ms . get_deployment ( my_model ) for deployment in list_deployments : print ( deployment . get_state ()) Arguments model Optional[hsml.model.Model] : Filter by model served in the deployments status Optional[str] : Filter by status of the deployments Returns List[Deployment] : A list of deployments. Raises RestAPIError : If unable to retrieve deployments from model serving. Properties # [source] artifact_path # Path of the model artifact deployed by the predictor. [source] artifact_version # Artifact version deployed by the predictor. [source] created_at # Created at date of the predictor. [source] creator # Creator of the predictor. [source] description # Description of the deployment. [source] id # Id of the deployment. [source] inference_batcher # Configuration of the inference batcher attached to this predictor. [source] inference_logger # Configuration of the inference logger attached to this predictor. [source] model_name # Name of the model deployed by the predictor [source] model_path # Model path deployed by the predictor. [source] model_server # Model server ran by the predictor. [source] model_version # Model version deployed by the predictor. [source] name # Name of the deployment. [source] predictor # Predictor used in the deployment. [source] requested_instances # Total number of requested instances in the deployment. [source] resources # Resource configuration for the predictor. [source] script_file # Script file used by the predictor. [source] serving_tool # Serving tool used to run the model server. [source] transformer # Transformer configured in the predictor. Methods # [source] delete # Deployment . delete ( force = False ) Delete the deployment Arguments force : Force the deletion of the deployment. If the deployment is running, it will be stopped and deleted automatically. !!! warn A call to this method does not ask for a second confirmation. [source] describe # Deployment . describe () Print a description of the deployment [source] download_artifact # Deployment . download_artifact () Download the model artifact served by the deployment [source] get_logs # Deployment . get_logs ( component = \"predictor\" , tail = 10 ) Prints the deployment logs of the predictor or transformer. Arguments component : Deployment component to get the logs from (e.g., predictor or transformer) tail : Number of most recent lines to retrieve from the logs. [source] get_state # Deployment . get_state () Get the current state of the deployment Returns PredictorState . The state of the deployment. [source] get_url # Deployment . get_url () Get url to the deployment in Hopsworks [source] is_created # Deployment . is_created () Check whether the deployment is created. Returns bool . Whether the deployment is created or not. [source] is_running # Deployment . is_running ( or_idle = True , or_updating = True ) Check whether the deployment is ready to handle inference requests Arguments or_idle : Whether the idle state is considered as running (default is True) or_updating : Whether the updating state is considered as running (default is True) Returns bool . Whether the deployment is ready or not. [source] is_stopped # Deployment . is_stopped ( or_created = True ) Check whether the deployment is stopped Arguments or_created : Whether the creating and created state is considered as stopped (default is True) Returns bool . Whether the deployment is stopped or not. [source] predict # Deployment . predict ( data = None , inputs = None ) Send inference requests to the deployment. One of data or inputs parameters must be set. If both are set, inputs will be ignored. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Serving handle ms = project . get_model_serving () # retrieve deployment by name my_deployment = ms . get_deployment ( \"my_deployment\" ) # (optional) retrieve model input example my_model = project . get_model_registry () . get_model ( my_deployment . model_name , my_deployment . model_version ) # make predictions using model inputs (single or batch) predictions = my_deployment . predict ( inputs = my_model . input_example ) # or using more sophisticated inference request payloads data = { \"instances\" : [ my_model . input_example ], \"key2\" : \"value2\" } predictions = my_deployment . predict ( data ) Arguments data Optional[dict] : Payload dictionary for the inference request including the model input(s) inputs Optional[list] : Model inputs used in the inference requests Returns dict . Inference response. [source] save # Deployment . save ( await_update = 60 ) Persist this deployment including the predictor and metadata to Model Serving. Arguments await_update Optional[int] : If the deployment is running, awaiting time (seconds) for the running instances to be updated. If the running instances are not updated within this timespan, the call to this method returns while the update in the background. [source] start # Deployment . start ( await_running = 60 ) Start the deployment Arguments await_running Optional[int] : Awaiting time (seconds) for the deployment to start. If the deployment has not started within this timespan, the call to this method returns while it deploys in the background. [source] stop # Deployment . stop ( await_stopped = 60 ) Stop the deployment Arguments await_stopped Optional[int] : Awaiting time (seconds) for the deployment to stop. If the deployment has not stopped within this timespan, the call to this method returns while it stopping in the background. [source] to_dict # Deployment . to_dict ()","title":"Deployment"},{"location":"generated/model-serving/deployment_api/#deployment","text":"","title":"Deployment"},{"location":"generated/model-serving/deployment_api/#handle","text":"[source]","title":"Handle"},{"location":"generated/model-serving/deployment_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Example import hopsworks project = hopsworks . login () ms = project . get_model_serving () Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/deployment_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/deployment_api/#create_deployment","text":"ModelServing . create_deployment ( predictor , name = None ) Create a Deployment metadata object. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () Using the model object # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) my_deployment = my_model . deploy () my_deployment . get_state () . describe () Using the Model Serving handle # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () my_deployment . get_state () . describe () Lazy This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the save() method. Arguments predictor hsml.predictor.Predictor : predictor to be used in the deployment name Optional[str] : name of the deployment Returns Deployment . The model metadata object. [source]","title":"create_deployment"},{"location":"generated/model-serving/deployment_api/#deploy","text":"Model . deploy ( name = None , description = None , artifact_version = \"CREATE\" , serving_tool = None , script_file = None , resources = None , inference_logger = None , inference_batcher = None , transformer = None , ) Deploy the model. Example import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) my_deployment = my_model . deploy () Arguments name Optional[str] : Name of the deployment. description Optional[str] : Description of the deployment. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Deployment : The deployment metadata object of a new or existing deployment. [source]","title":"deploy"},{"location":"generated/model-serving/deployment_api/#deploy_1","text":"Predictor . deploy () Create a deployment for this predictor and persists it in the Model Serving. Example import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () print ( my_deployment . get_state ()) Returns Deployment . The deployment metadata object of a new or existing deployment.","title":"deploy"},{"location":"generated/model-serving/deployment_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-serving/deployment_api/#get_deployment","text":"ModelServing . get_deployment ( name ) Get a deployment by name from Model Serving. Example # login and get Hopsworks Model Serving handle using .login() and .get_model_serving() # get a deployment by name my_deployment = ms . get_deployment ( 'deployment_name' ) Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments name str : Name of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source]","title":"get_deployment"},{"location":"generated/model-serving/deployment_api/#get_deployment_by_id","text":"ModelServing . get_deployment_by_id ( id ) Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Example # login and get Hopsworks Model Serving handle using .login() and .get_model_serving() # get a deployment by id my_deployment = ms . get_deployment_by_id ( 1 ) Arguments id int : Id of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source]","title":"get_deployment_by_id"},{"location":"generated/model-serving/deployment_api/#get_deployments","text":"ModelServing . get_deployments ( model = None , status = None ) Get all deployments from model serving. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) list_deployments = ms . get_deployment ( my_model ) for deployment in list_deployments : print ( deployment . get_state ()) Arguments model Optional[hsml.model.Model] : Filter by model served in the deployments status Optional[str] : Filter by status of the deployments Returns List[Deployment] : A list of deployments. Raises RestAPIError : If unable to retrieve deployments from model serving.","title":"get_deployments"},{"location":"generated/model-serving/deployment_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/deployment_api/#artifact_path","text":"Path of the model artifact deployed by the predictor. [source]","title":"artifact_path"},{"location":"generated/model-serving/deployment_api/#artifact_version","text":"Artifact version deployed by the predictor. [source]","title":"artifact_version"},{"location":"generated/model-serving/deployment_api/#created_at","text":"Created at date of the predictor. [source]","title":"created_at"},{"location":"generated/model-serving/deployment_api/#creator","text":"Creator of the predictor. [source]","title":"creator"},{"location":"generated/model-serving/deployment_api/#description","text":"Description of the deployment. [source]","title":"description"},{"location":"generated/model-serving/deployment_api/#id","text":"Id of the deployment. [source]","title":"id"},{"location":"generated/model-serving/deployment_api/#inference_batcher","text":"Configuration of the inference batcher attached to this predictor. [source]","title":"inference_batcher"},{"location":"generated/model-serving/deployment_api/#inference_logger","text":"Configuration of the inference logger attached to this predictor. [source]","title":"inference_logger"},{"location":"generated/model-serving/deployment_api/#model_name","text":"Name of the model deployed by the predictor [source]","title":"model_name"},{"location":"generated/model-serving/deployment_api/#model_path","text":"Model path deployed by the predictor. [source]","title":"model_path"},{"location":"generated/model-serving/deployment_api/#model_server","text":"Model server ran by the predictor. [source]","title":"model_server"},{"location":"generated/model-serving/deployment_api/#model_version","text":"Model version deployed by the predictor. [source]","title":"model_version"},{"location":"generated/model-serving/deployment_api/#name","text":"Name of the deployment. [source]","title":"name"},{"location":"generated/model-serving/deployment_api/#predictor","text":"Predictor used in the deployment. [source]","title":"predictor"},{"location":"generated/model-serving/deployment_api/#requested_instances","text":"Total number of requested instances in the deployment. [source]","title":"requested_instances"},{"location":"generated/model-serving/deployment_api/#resources","text":"Resource configuration for the predictor. [source]","title":"resources"},{"location":"generated/model-serving/deployment_api/#script_file","text":"Script file used by the predictor. [source]","title":"script_file"},{"location":"generated/model-serving/deployment_api/#serving_tool","text":"Serving tool used to run the model server. [source]","title":"serving_tool"},{"location":"generated/model-serving/deployment_api/#transformer","text":"Transformer configured in the predictor.","title":"transformer"},{"location":"generated/model-serving/deployment_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/deployment_api/#delete","text":"Deployment . delete ( force = False ) Delete the deployment Arguments force : Force the deletion of the deployment. If the deployment is running, it will be stopped and deleted automatically. !!! warn A call to this method does not ask for a second confirmation. [source]","title":"delete"},{"location":"generated/model-serving/deployment_api/#describe","text":"Deployment . describe () Print a description of the deployment [source]","title":"describe"},{"location":"generated/model-serving/deployment_api/#download_artifact","text":"Deployment . download_artifact () Download the model artifact served by the deployment [source]","title":"download_artifact"},{"location":"generated/model-serving/deployment_api/#get_logs","text":"Deployment . get_logs ( component = \"predictor\" , tail = 10 ) Prints the deployment logs of the predictor or transformer. Arguments component : Deployment component to get the logs from (e.g., predictor or transformer) tail : Number of most recent lines to retrieve from the logs. [source]","title":"get_logs"},{"location":"generated/model-serving/deployment_api/#get_state","text":"Deployment . get_state () Get the current state of the deployment Returns PredictorState . The state of the deployment. [source]","title":"get_state"},{"location":"generated/model-serving/deployment_api/#get_url","text":"Deployment . get_url () Get url to the deployment in Hopsworks [source]","title":"get_url"},{"location":"generated/model-serving/deployment_api/#is_created","text":"Deployment . is_created () Check whether the deployment is created. Returns bool . Whether the deployment is created or not. [source]","title":"is_created"},{"location":"generated/model-serving/deployment_api/#is_running","text":"Deployment . is_running ( or_idle = True , or_updating = True ) Check whether the deployment is ready to handle inference requests Arguments or_idle : Whether the idle state is considered as running (default is True) or_updating : Whether the updating state is considered as running (default is True) Returns bool . Whether the deployment is ready or not. [source]","title":"is_running"},{"location":"generated/model-serving/deployment_api/#is_stopped","text":"Deployment . is_stopped ( or_created = True ) Check whether the deployment is stopped Arguments or_created : Whether the creating and created state is considered as stopped (default is True) Returns bool . Whether the deployment is stopped or not. [source]","title":"is_stopped"},{"location":"generated/model-serving/deployment_api/#predict","text":"Deployment . predict ( data = None , inputs = None ) Send inference requests to the deployment. One of data or inputs parameters must be set. If both are set, inputs will be ignored. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Serving handle ms = project . get_model_serving () # retrieve deployment by name my_deployment = ms . get_deployment ( \"my_deployment\" ) # (optional) retrieve model input example my_model = project . get_model_registry () . get_model ( my_deployment . model_name , my_deployment . model_version ) # make predictions using model inputs (single or batch) predictions = my_deployment . predict ( inputs = my_model . input_example ) # or using more sophisticated inference request payloads data = { \"instances\" : [ my_model . input_example ], \"key2\" : \"value2\" } predictions = my_deployment . predict ( data ) Arguments data Optional[dict] : Payload dictionary for the inference request including the model input(s) inputs Optional[list] : Model inputs used in the inference requests Returns dict . Inference response. [source]","title":"predict"},{"location":"generated/model-serving/deployment_api/#save","text":"Deployment . save ( await_update = 60 ) Persist this deployment including the predictor and metadata to Model Serving. Arguments await_update Optional[int] : If the deployment is running, awaiting time (seconds) for the running instances to be updated. If the running instances are not updated within this timespan, the call to this method returns while the update in the background. [source]","title":"save"},{"location":"generated/model-serving/deployment_api/#start","text":"Deployment . start ( await_running = 60 ) Start the deployment Arguments await_running Optional[int] : Awaiting time (seconds) for the deployment to start. If the deployment has not started within this timespan, the call to this method returns while it deploys in the background. [source]","title":"start"},{"location":"generated/model-serving/deployment_api/#stop","text":"Deployment . stop ( await_stopped = 60 ) Stop the deployment Arguments await_stopped Optional[int] : Awaiting time (seconds) for the deployment to stop. If the deployment has not stopped within this timespan, the call to this method returns while it stopping in the background. [source]","title":"stop"},{"location":"generated/model-serving/deployment_api/#to_dict","text":"Deployment . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/inference_batcher_api/","text":"Inference batcher # Creation # [source] InferenceBatcher # hsml . inference_batcher . InferenceBatcher ( enabled = None , max_batch_size = None , max_latency = None , timeout = None , ** kwargs ) Configuration of an inference batcher for a predictor. Arguments enabled Optional[bool] : Whether the inference batcher is enabled or not. The default value is false . max_batch_size Optional[int] : Maximum requests batch size. max_latency Optional[int] : Maximum latency for request batching. timeout Optional[int] : Maximum waiting time for request batching. Returns InferenceLogger . Configuration of an inference logger. Retrieval # predictor.inference_batcher # Inference batchers can be accessed from the predictor metadata objects. predictor . inference_batcher Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference . Properties # [source] enabled # Whether the inference batcher is enabled or not. [source] max_batch_size # Maximum requests batch size. [source] max_latency # Maximum latency. [source] timeout # Maximum timeout. Methods # [source] describe # InferenceBatcher . describe () Print a description of the inference batcher [source] to_dict # InferenceBatcher . to_dict ()","title":"Inference Batcher"},{"location":"generated/model-serving/inference_batcher_api/#inference-batcher","text":"","title":"Inference batcher"},{"location":"generated/model-serving/inference_batcher_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/inference_batcher_api/#inferencebatcher","text":"hsml . inference_batcher . InferenceBatcher ( enabled = None , max_batch_size = None , max_latency = None , timeout = None , ** kwargs ) Configuration of an inference batcher for a predictor. Arguments enabled Optional[bool] : Whether the inference batcher is enabled or not. The default value is false . max_batch_size Optional[int] : Maximum requests batch size. max_latency Optional[int] : Maximum latency for request batching. timeout Optional[int] : Maximum waiting time for request batching. Returns InferenceLogger . Configuration of an inference logger.","title":"InferenceBatcher"},{"location":"generated/model-serving/inference_batcher_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-serving/inference_batcher_api/#predictorinference_batcher","text":"Inference batchers can be accessed from the predictor metadata objects. predictor . inference_batcher Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference .","title":"predictor.inference_batcher"},{"location":"generated/model-serving/inference_batcher_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/inference_batcher_api/#enabled","text":"Whether the inference batcher is enabled or not. [source]","title":"enabled"},{"location":"generated/model-serving/inference_batcher_api/#max_batch_size","text":"Maximum requests batch size. [source]","title":"max_batch_size"},{"location":"generated/model-serving/inference_batcher_api/#max_latency","text":"Maximum latency. [source]","title":"max_latency"},{"location":"generated/model-serving/inference_batcher_api/#timeout","text":"Maximum timeout.","title":"timeout"},{"location":"generated/model-serving/inference_batcher_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/inference_batcher_api/#describe","text":"InferenceBatcher . describe () Print a description of the inference batcher [source]","title":"describe"},{"location":"generated/model-serving/inference_batcher_api/#to_dict","text":"InferenceBatcher . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/inference_logger_api/","text":"Inference logger # Creation # [source] InferenceLogger # hsml . inference_logger . InferenceLogger ( kafka_topic = {}, mode = \"ALL\" , ** kwargs ) Configuration of an inference logger for a predictor. Arguments kafka_topic Optional[Union[hsml.kafka_topic.KafkaTopic, dict]] : Kafka topic to send the inference logs to. By default, a new Kafka topic is configured. mode Optional[str] : Inference logging mode. (e.g., NONE , ALL , PREDICTIONS , or MODEL_INPUTS ). By default, ALL inference logs are sent. Returns InferenceLogger . Configuration of an inference logger. Retrieval # predictor.inference_logger # Inference loggers can be accessed from the predictor metadata objects. predictor . inference_logger Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference . Properties # [source] kafka_topic # Kafka topic to send the inference logs to. [source] mode # Inference logging mode (\"NONE\", \"ALL\", \"PREDICTIONS\", or \"MODEL_INPUTS\"). Methods # [source] describe # InferenceLogger . describe () Print a description of the inference logger [source] to_dict # InferenceLogger . to_dict ()","title":"Inference Logger"},{"location":"generated/model-serving/inference_logger_api/#inference-logger","text":"","title":"Inference logger"},{"location":"generated/model-serving/inference_logger_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/inference_logger_api/#inferencelogger","text":"hsml . inference_logger . InferenceLogger ( kafka_topic = {}, mode = \"ALL\" , ** kwargs ) Configuration of an inference logger for a predictor. Arguments kafka_topic Optional[Union[hsml.kafka_topic.KafkaTopic, dict]] : Kafka topic to send the inference logs to. By default, a new Kafka topic is configured. mode Optional[str] : Inference logging mode. (e.g., NONE , ALL , PREDICTIONS , or MODEL_INPUTS ). By default, ALL inference logs are sent. Returns InferenceLogger . Configuration of an inference logger.","title":"InferenceLogger"},{"location":"generated/model-serving/inference_logger_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-serving/inference_logger_api/#predictorinference_logger","text":"Inference loggers can be accessed from the predictor metadata objects. predictor . inference_logger Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference .","title":"predictor.inference_logger"},{"location":"generated/model-serving/inference_logger_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/inference_logger_api/#kafka_topic","text":"Kafka topic to send the inference logs to. [source]","title":"kafka_topic"},{"location":"generated/model-serving/inference_logger_api/#mode","text":"Inference logging mode (\"NONE\", \"ALL\", \"PREDICTIONS\", or \"MODEL_INPUTS\").","title":"mode"},{"location":"generated/model-serving/inference_logger_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/inference_logger_api/#describe","text":"InferenceLogger . describe () Print a description of the inference logger [source]","title":"describe"},{"location":"generated/model-serving/inference_logger_api/#to_dict","text":"InferenceLogger . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/model_serving_api/","text":"Model Serving # Retrieval # [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Example import hopsworks project = hopsworks . login () ms = project . get_model_serving () Returns ModelServing . A model serving handle object to perform operations on. Properties # [source] project_id # Id of the project in which Model Serving is located. [source] project_name # Name of the project in which Model Serving is located. [source] project_path # Path of the project the registry is connected to. Methods # [source] create_deployment # ModelServing . create_deployment ( predictor , name = None ) Create a Deployment metadata object. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () Using the model object # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) my_deployment = my_model . deploy () my_deployment . get_state () . describe () Using the Model Serving handle # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () my_deployment . get_state () . describe () Lazy This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the save() method. Arguments predictor hsml.predictor.Predictor : predictor to be used in the deployment name Optional[str] : name of the deployment Returns Deployment . The model metadata object. [source] create_predictor # ModelServing . create_predictor ( model , name = None , artifact_version = \"CREATE\" , serving_tool = None , script_file = None , resources = None , inference_logger = None , inference_batcher = None , transformer = None , ) Create a Predictor metadata object. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () Lazy This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the deploy() method. Arguments model hsml.model.Model : Model to be deployed. name Optional[str] : Name of the predictor. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict, str]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Predictor . The predictor metadata object. [source] create_transformer # ModelServing . create_transformer ( script_file = None , resources = None ) Create a Transformer metadata object. Example # login into Hopsworks using hopsworks.login() # get Dataset API instance dataset_api = project . get_dataset_api () # get Hopsworks Model Serving handle ms = project . get_model_serving () # create my_transformer.py Python script class Transformer ( object ): def __init__ ( self ): ''' Initialization code goes here ''' pass def preprocess ( self , inputs ): ''' Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. ''' return inputs def postprocess ( self , outputs ): ''' Transform the predictions computed by the model before returning a response ''' return outputs uploaded_file_path = dataset_api . upload ( \"my_transformer.py\" , \"Resources\" , overwrite = True ) transformer_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path ) my_transformer = ms . create_transformer ( script_file = uploaded_file_path ) # or from hsml.transformer import Transformer my_transformer = Transformer ( script_file ) Create a deployment with the transformer my_predictor = ms . create_predictor ( transformer = my_transformer ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor , transformer = my_transformer ) my_deployment . save () Lazy This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the predictor.transformer property. Arguments script_file Optional[str] : Path to a custom predictor script implementing the Transformer class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the transformer. Returns Transformer . The model metadata object. [source] get_deployment # ModelServing . get_deployment ( name ) Get a deployment by name from Model Serving. Example # login and get Hopsworks Model Serving handle using .login() and .get_model_serving() # get a deployment by name my_deployment = ms . get_deployment ( 'deployment_name' ) Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments name str : Name of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source] get_deployment_by_id # ModelServing . get_deployment_by_id ( id ) Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Example # login and get Hopsworks Model Serving handle using .login() and .get_model_serving() # get a deployment by id my_deployment = ms . get_deployment_by_id ( 1 ) Arguments id int : Id of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source] get_deployments # ModelServing . get_deployments ( model = None , status = None ) Get all deployments from model serving. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) list_deployments = ms . get_deployment ( my_model ) for deployment in list_deployments : print ( deployment . get_state ()) Arguments model Optional[hsml.model.Model] : Filter by model served in the deployments status Optional[str] : Filter by status of the deployments Returns List[Deployment] : A list of deployments. Raises RestAPIError : If unable to retrieve deployments from model serving. [source] get_inference_endpoints # ModelServing . get_inference_endpoints () Get all inference endpoints available in the current project. Returns List[InferenceEndpoint] : Inference endpoints for model inference","title":"Model Serving"},{"location":"generated/model-serving/model_serving_api/#model-serving","text":"","title":"Model Serving"},{"location":"generated/model-serving/model_serving_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-serving/model_serving_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Example import hopsworks project = hopsworks . login () ms = project . get_model_serving () Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/model_serving_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/model_serving_api/#project_id","text":"Id of the project in which Model Serving is located. [source]","title":"project_id"},{"location":"generated/model-serving/model_serving_api/#project_name","text":"Name of the project in which Model Serving is located. [source]","title":"project_name"},{"location":"generated/model-serving/model_serving_api/#project_path","text":"Path of the project the registry is connected to.","title":"project_path"},{"location":"generated/model-serving/model_serving_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/model_serving_api/#create_deployment","text":"ModelServing . create_deployment ( predictor , name = None ) Create a Deployment metadata object. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () Using the model object # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) my_deployment = my_model . deploy () my_deployment . get_state () . describe () Using the Model Serving handle # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () my_deployment . get_state () . describe () Lazy This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the save() method. Arguments predictor hsml.predictor.Predictor : predictor to be used in the deployment name Optional[str] : name of the deployment Returns Deployment . The model metadata object. [source]","title":"create_deployment"},{"location":"generated/model-serving/model_serving_api/#create_predictor","text":"ModelServing . create_predictor ( model , name = None , artifact_version = \"CREATE\" , serving_tool = None , script_file = None , resources = None , inference_logger = None , inference_batcher = None , transformer = None , ) Create a Predictor metadata object. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () Lazy This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the deploy() method. Arguments model hsml.model.Model : Model to be deployed. name Optional[str] : Name of the predictor. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict, str]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Predictor . The predictor metadata object. [source]","title":"create_predictor"},{"location":"generated/model-serving/model_serving_api/#create_transformer","text":"ModelServing . create_transformer ( script_file = None , resources = None ) Create a Transformer metadata object. Example # login into Hopsworks using hopsworks.login() # get Dataset API instance dataset_api = project . get_dataset_api () # get Hopsworks Model Serving handle ms = project . get_model_serving () # create my_transformer.py Python script class Transformer ( object ): def __init__ ( self ): ''' Initialization code goes here ''' pass def preprocess ( self , inputs ): ''' Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. ''' return inputs def postprocess ( self , outputs ): ''' Transform the predictions computed by the model before returning a response ''' return outputs uploaded_file_path = dataset_api . upload ( \"my_transformer.py\" , \"Resources\" , overwrite = True ) transformer_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path ) my_transformer = ms . create_transformer ( script_file = uploaded_file_path ) # or from hsml.transformer import Transformer my_transformer = Transformer ( script_file ) Create a deployment with the transformer my_predictor = ms . create_predictor ( transformer = my_transformer ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor , transformer = my_transformer ) my_deployment . save () Lazy This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the predictor.transformer property. Arguments script_file Optional[str] : Path to a custom predictor script implementing the Transformer class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the transformer. Returns Transformer . The model metadata object. [source]","title":"create_transformer"},{"location":"generated/model-serving/model_serving_api/#get_deployment","text":"ModelServing . get_deployment ( name ) Get a deployment by name from Model Serving. Example # login and get Hopsworks Model Serving handle using .login() and .get_model_serving() # get a deployment by name my_deployment = ms . get_deployment ( 'deployment_name' ) Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments name str : Name of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source]","title":"get_deployment"},{"location":"generated/model-serving/model_serving_api/#get_deployment_by_id","text":"ModelServing . get_deployment_by_id ( id ) Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Example # login and get Hopsworks Model Serving handle using .login() and .get_model_serving() # get a deployment by id my_deployment = ms . get_deployment_by_id ( 1 ) Arguments id int : Id of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source]","title":"get_deployment_by_id"},{"location":"generated/model-serving/model_serving_api/#get_deployments","text":"ModelServing . get_deployments ( model = None , status = None ) Get all deployments from model serving. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) list_deployments = ms . get_deployment ( my_model ) for deployment in list_deployments : print ( deployment . get_state ()) Arguments model Optional[hsml.model.Model] : Filter by model served in the deployments status Optional[str] : Filter by status of the deployments Returns List[Deployment] : A list of deployments. Raises RestAPIError : If unable to retrieve deployments from model serving. [source]","title":"get_deployments"},{"location":"generated/model-serving/model_serving_api/#get_inference_endpoints","text":"ModelServing . get_inference_endpoints () Get all inference endpoints available in the current project. Returns List[InferenceEndpoint] : Inference endpoints for model inference","title":"get_inference_endpoints"},{"location":"generated/model-serving/predictor_api/","text":"Predictor # Handle # [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Example import hopsworks project = hopsworks . login () ms = project . get_model_serving () Returns ModelServing . A model serving handle object to perform operations on. Creation # [source] create_predictor # ModelServing . create_predictor ( model , name = None , artifact_version = \"CREATE\" , serving_tool = None , script_file = None , resources = None , inference_logger = None , inference_batcher = None , transformer = None , ) Create a Predictor metadata object. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () Lazy This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the deploy() method. Arguments model hsml.model.Model : Model to be deployed. name Optional[str] : Name of the predictor. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict, str]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Predictor . The predictor metadata object. Retrieval # deployment.predictor # Predictors can be accessed from the deployment metadata objects. deployment . predictor To retrieve a deployment, see the Deployment Reference . Properties # [source] artifact_path # Path of the model artifact deployed by the predictor. Resolves to /Projects/{project_name}/Models/{name}/{version}/Artifacts/{artifact_version}/{name} {version} .zip [source] artifact_version # Artifact version deployed by the predictor. [source] created_at # Created at date of the predictor. [source] creator # Creator of the predictor. [source] description # Description of the predictor. [source] id # Id of the predictor. [source] inference_batcher # Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer). [source] inference_logger # Configuration of the inference logger attached to this predictor. [source] model_framework # Model framework of the model to be deployed by the predictor. [source] model_name # Name of the model deployed by the predictor. [source] model_path # Model path deployed by the predictor. [source] model_server # Model server used by the predictor. [source] model_version # Model version deployed by the predictor. [source] name # Name of the predictor. [source] requested_instances # Total number of requested instances in the predictor. [source] resources # Resource configuration for the deployment component (i.e., predictor or transformer). [source] script_file # Script file used to load and run the model. [source] serving_tool # Serving tool used to run the model server. [source] transformer # Transformer configuration attached to the predictor. Methods # [source] deploy # Predictor . deploy () Create a deployment for this predictor and persists it in the Model Serving. Example import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () print ( my_deployment . get_state ()) Returns Deployment . The deployment metadata object of a new or existing deployment. [source] describe # Predictor . describe () Print a description of the predictor [source] to_dict # Predictor . to_dict () To be implemented by the component type","title":"Predictor"},{"location":"generated/model-serving/predictor_api/#predictor","text":"","title":"Predictor"},{"location":"generated/model-serving/predictor_api/#handle","text":"[source]","title":"Handle"},{"location":"generated/model-serving/predictor_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Example import hopsworks project = hopsworks . login () ms = project . get_model_serving () Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/predictor_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/predictor_api/#create_predictor","text":"ModelServing . create_predictor ( model , name = None , artifact_version = \"CREATE\" , serving_tool = None , script_file = None , resources = None , inference_logger = None , inference_batcher = None , transformer = None , ) Create a Predictor metadata object. Example # login into Hopsworks using hopsworks.login() # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () Lazy This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the deploy() method. Arguments model hsml.model.Model : Model to be deployed. name Optional[str] : Name of the predictor. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict, str]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Predictor . The predictor metadata object.","title":"create_predictor"},{"location":"generated/model-serving/predictor_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-serving/predictor_api/#deploymentpredictor","text":"Predictors can be accessed from the deployment metadata objects. deployment . predictor To retrieve a deployment, see the Deployment Reference .","title":"deployment.predictor"},{"location":"generated/model-serving/predictor_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/predictor_api/#artifact_path","text":"Path of the model artifact deployed by the predictor. Resolves to /Projects/{project_name}/Models/{name}/{version}/Artifacts/{artifact_version}/{name} {version} .zip [source]","title":"artifact_path"},{"location":"generated/model-serving/predictor_api/#artifact_version","text":"Artifact version deployed by the predictor. [source]","title":"artifact_version"},{"location":"generated/model-serving/predictor_api/#created_at","text":"Created at date of the predictor. [source]","title":"created_at"},{"location":"generated/model-serving/predictor_api/#creator","text":"Creator of the predictor. [source]","title":"creator"},{"location":"generated/model-serving/predictor_api/#description","text":"Description of the predictor. [source]","title":"description"},{"location":"generated/model-serving/predictor_api/#id","text":"Id of the predictor. [source]","title":"id"},{"location":"generated/model-serving/predictor_api/#inference_batcher","text":"Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer). [source]","title":"inference_batcher"},{"location":"generated/model-serving/predictor_api/#inference_logger","text":"Configuration of the inference logger attached to this predictor. [source]","title":"inference_logger"},{"location":"generated/model-serving/predictor_api/#model_framework","text":"Model framework of the model to be deployed by the predictor. [source]","title":"model_framework"},{"location":"generated/model-serving/predictor_api/#model_name","text":"Name of the model deployed by the predictor. [source]","title":"model_name"},{"location":"generated/model-serving/predictor_api/#model_path","text":"Model path deployed by the predictor. [source]","title":"model_path"},{"location":"generated/model-serving/predictor_api/#model_server","text":"Model server used by the predictor. [source]","title":"model_server"},{"location":"generated/model-serving/predictor_api/#model_version","text":"Model version deployed by the predictor. [source]","title":"model_version"},{"location":"generated/model-serving/predictor_api/#name","text":"Name of the predictor. [source]","title":"name"},{"location":"generated/model-serving/predictor_api/#requested_instances","text":"Total number of requested instances in the predictor. [source]","title":"requested_instances"},{"location":"generated/model-serving/predictor_api/#resources","text":"Resource configuration for the deployment component (i.e., predictor or transformer). [source]","title":"resources"},{"location":"generated/model-serving/predictor_api/#script_file","text":"Script file used to load and run the model. [source]","title":"script_file"},{"location":"generated/model-serving/predictor_api/#serving_tool","text":"Serving tool used to run the model server. [source]","title":"serving_tool"},{"location":"generated/model-serving/predictor_api/#transformer","text":"Transformer configuration attached to the predictor.","title":"transformer"},{"location":"generated/model-serving/predictor_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/predictor_api/#deploy","text":"Predictor . deploy () Create a deployment for this predictor and persists it in the Model Serving. Example import hopsworks project = hopsworks . login () # get Hopsworks Model Registry handle mr = project . get_model_registry () # retrieve the trained model you want to deploy my_model = mr . get_model ( \"my_model\" , version = 1 ) # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () print ( my_deployment . get_state ()) Returns Deployment . The deployment metadata object of a new or existing deployment. [source]","title":"deploy"},{"location":"generated/model-serving/predictor_api/#describe","text":"Predictor . describe () Print a description of the predictor [source]","title":"describe"},{"location":"generated/model-serving/predictor_api/#to_dict","text":"Predictor . to_dict () To be implemented by the component type","title":"to_dict"},{"location":"generated/model-serving/predictor_state_api/","text":"Deployment state # The state of a deployment corresponds to the state of the predictor configured in it. Note Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon. Retrieval # [source] get_state # Deployment . get_state () Get the current state of the deployment Returns PredictorState . The state of the deployment. Properties # [source] available_predictor_instances # Available predicotr instances. [source] available_transformer_instances # Available transformer instances. [source] condition # Condition of the current state of predictor. [source] deployed # Whether the predictor is deployed or not. [source] hopsworks_inference_path # Inference path in the Hopsworks REST API. [source] internal_port # Internal port for the predictor. [source] model_server_inference_path # Inference path in the model server [source] revision # Last revision of the predictor. [source] status # Status of the predictor. Methods # [source] describe # PredictorState . describe () Print a description of the deployment state [source] to_dict # PredictorState . to_dict ()","title":"Deployment state"},{"location":"generated/model-serving/predictor_state_api/#deployment-state","text":"The state of a deployment corresponds to the state of the predictor configured in it. Note Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon.","title":"Deployment state"},{"location":"generated/model-serving/predictor_state_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-serving/predictor_state_api/#get_state","text":"Deployment . get_state () Get the current state of the deployment Returns PredictorState . The state of the deployment.","title":"get_state"},{"location":"generated/model-serving/predictor_state_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/predictor_state_api/#available_predictor_instances","text":"Available predicotr instances. [source]","title":"available_predictor_instances"},{"location":"generated/model-serving/predictor_state_api/#available_transformer_instances","text":"Available transformer instances. [source]","title":"available_transformer_instances"},{"location":"generated/model-serving/predictor_state_api/#condition","text":"Condition of the current state of predictor. [source]","title":"condition"},{"location":"generated/model-serving/predictor_state_api/#deployed","text":"Whether the predictor is deployed or not. [source]","title":"deployed"},{"location":"generated/model-serving/predictor_state_api/#hopsworks_inference_path","text":"Inference path in the Hopsworks REST API. [source]","title":"hopsworks_inference_path"},{"location":"generated/model-serving/predictor_state_api/#internal_port","text":"Internal port for the predictor. [source]","title":"internal_port"},{"location":"generated/model-serving/predictor_state_api/#model_server_inference_path","text":"Inference path in the model server [source]","title":"model_server_inference_path"},{"location":"generated/model-serving/predictor_state_api/#revision","text":"Last revision of the predictor. [source]","title":"revision"},{"location":"generated/model-serving/predictor_state_api/#status","text":"Status of the predictor.","title":"status"},{"location":"generated/model-serving/predictor_state_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/predictor_state_api/#describe","text":"PredictorState . describe () Print a description of the deployment state [source]","title":"describe"},{"location":"generated/model-serving/predictor_state_api/#to_dict","text":"PredictorState . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/predictor_state_condition_api/","text":"Deployment state condition # The state condition of a deployment is a more detailed representation of a deployment state. Retrieval # [source] condition # Condition of the current state of predictor. Properties # [source] reason # Condition reason of the predictor state. [source] status # Condition status of the predictor state. [source] type # Condition type of the predictor state. Methods # [source] describe # PredictorStateCondition . describe () [source] to_dict # PredictorStateCondition . to_dict ()","title":"Deployment state condition"},{"location":"generated/model-serving/predictor_state_condition_api/#deployment-state-condition","text":"The state condition of a deployment is a more detailed representation of a deployment state.","title":"Deployment state condition"},{"location":"generated/model-serving/predictor_state_condition_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-serving/predictor_state_condition_api/#condition","text":"Condition of the current state of predictor.","title":"condition"},{"location":"generated/model-serving/predictor_state_condition_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/predictor_state_condition_api/#reason","text":"Condition reason of the predictor state. [source]","title":"reason"},{"location":"generated/model-serving/predictor_state_condition_api/#status","text":"Condition status of the predictor state. [source]","title":"status"},{"location":"generated/model-serving/predictor_state_condition_api/#type","text":"Condition type of the predictor state.","title":"type"},{"location":"generated/model-serving/predictor_state_condition_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/predictor_state_condition_api/#describe","text":"PredictorStateCondition . describe () [source]","title":"describe"},{"location":"generated/model-serving/predictor_state_condition_api/#to_dict","text":"PredictorStateCondition . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/resources_api/","text":"Resources # Creation # [source] Resources # hsml . resources . Resources ( cores , memory , gpus , ** kwargs ) Resource configuration for a predictor or transformer. Arguments cores int : Number of CPUs. memory int : Memory (MB) resources. gpus int : Number of GPUs. Returns Resources . Resource configuration for a predictor or transformer. Retrieval # predictor.resources # Resources allocated for a preditor can be accessed from the predictor metadata object. predictor . resources Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference . transformer.resources # Resources allocated for a transformer can be accessed from the transformer metadata object. transformer . resources Transformer can be found in the predictor metadata objects (see Predictor Reference ). Properties # [source] cores # Number of CPUs to be allocated per instance [source] gpus # Number of GPUs to be allocated per instance [source] memory # Memory resources to be allocated per instance Methods # [source] describe # Resources . describe () Print a description of the resource configuration [source] to_dict # Resources . to_dict ()","title":"Resources"},{"location":"generated/model-serving/resources_api/#resources","text":"","title":"Resources"},{"location":"generated/model-serving/resources_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/resources_api/#resources_1","text":"hsml . resources . Resources ( cores , memory , gpus , ** kwargs ) Resource configuration for a predictor or transformer. Arguments cores int : Number of CPUs. memory int : Memory (MB) resources. gpus int : Number of GPUs. Returns Resources . Resource configuration for a predictor or transformer.","title":"Resources"},{"location":"generated/model-serving/resources_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-serving/resources_api/#predictorresources","text":"Resources allocated for a preditor can be accessed from the predictor metadata object. predictor . resources Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference .","title":"predictor.resources"},{"location":"generated/model-serving/resources_api/#transformerresources","text":"Resources allocated for a transformer can be accessed from the transformer metadata object. transformer . resources Transformer can be found in the predictor metadata objects (see Predictor Reference ).","title":"transformer.resources"},{"location":"generated/model-serving/resources_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/resources_api/#cores","text":"Number of CPUs to be allocated per instance [source]","title":"cores"},{"location":"generated/model-serving/resources_api/#gpus","text":"Number of GPUs to be allocated per instance [source]","title":"gpus"},{"location":"generated/model-serving/resources_api/#memory","text":"Memory resources to be allocated per instance","title":"memory"},{"location":"generated/model-serving/resources_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/resources_api/#describe","text":"Resources . describe () Print a description of the resource configuration [source]","title":"describe"},{"location":"generated/model-serving/resources_api/#to_dict","text":"Resources . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/transformer_api/","text":"Transformer # Handle # [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Example import hopsworks project = hopsworks . login () ms = project . get_model_serving () Returns ModelServing . A model serving handle object to perform operations on. Creation # [source] create_transformer # ModelServing . create_transformer ( script_file = None , resources = None ) Create a Transformer metadata object. Example # login into Hopsworks using hopsworks.login() # get Dataset API instance dataset_api = project . get_dataset_api () # get Hopsworks Model Serving handle ms = project . get_model_serving () # create my_transformer.py Python script class Transformer ( object ): def __init__ ( self ): ''' Initialization code goes here ''' pass def preprocess ( self , inputs ): ''' Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. ''' return inputs def postprocess ( self , outputs ): ''' Transform the predictions computed by the model before returning a response ''' return outputs uploaded_file_path = dataset_api . upload ( \"my_transformer.py\" , \"Resources\" , overwrite = True ) transformer_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path ) my_transformer = ms . create_transformer ( script_file = uploaded_file_path ) # or from hsml.transformer import Transformer my_transformer = Transformer ( script_file ) Create a deployment with the transformer my_predictor = ms . create_predictor ( transformer = my_transformer ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor , transformer = my_transformer ) my_deployment . save () Lazy This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the predictor.transformer property. Arguments script_file Optional[str] : Path to a custom predictor script implementing the Transformer class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the transformer. Returns Transformer . The model metadata object. Retrieval # predictor.transformer # Transformers can be accessed from the predictor metadata objects. predictor . transformer Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference . Properties # [source] inference_batcher # Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer). [source] resources # Resource configuration for the deployment component (i.e., predictor or transformer). [source] script_file # Script file ran by the deployment component (i.e., predictor or transformer). Methods # [source] describe # Transformer . describe () Print a description of the transformer [source] to_dict # Transformer . to_dict () To be implemented by the component type","title":"Transformer"},{"location":"generated/model-serving/transformer_api/#transformer","text":"","title":"Transformer"},{"location":"generated/model-serving/transformer_api/#handle","text":"[source]","title":"Handle"},{"location":"generated/model-serving/transformer_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Example import hopsworks project = hopsworks . login () ms = project . get_model_serving () Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/transformer_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/transformer_api/#create_transformer","text":"ModelServing . create_transformer ( script_file = None , resources = None ) Create a Transformer metadata object. Example # login into Hopsworks using hopsworks.login() # get Dataset API instance dataset_api = project . get_dataset_api () # get Hopsworks Model Serving handle ms = project . get_model_serving () # create my_transformer.py Python script class Transformer ( object ): def __init__ ( self ): ''' Initialization code goes here ''' pass def preprocess ( self , inputs ): ''' Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. ''' return inputs def postprocess ( self , outputs ): ''' Transform the predictions computed by the model before returning a response ''' return outputs uploaded_file_path = dataset_api . upload ( \"my_transformer.py\" , \"Resources\" , overwrite = True ) transformer_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path ) my_transformer = ms . create_transformer ( script_file = uploaded_file_path ) # or from hsml.transformer import Transformer my_transformer = Transformer ( script_file ) Create a deployment with the transformer my_predictor = ms . create_predictor ( transformer = my_transformer ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor , transformer = my_transformer ) my_deployment . save () Lazy This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the predictor.transformer property. Arguments script_file Optional[str] : Path to a custom predictor script implementing the Transformer class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the transformer. Returns Transformer . The model metadata object.","title":"create_transformer"},{"location":"generated/model-serving/transformer_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-serving/transformer_api/#predictortransformer","text":"Transformers can be accessed from the predictor metadata objects. predictor . transformer Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference .","title":"predictor.transformer"},{"location":"generated/model-serving/transformer_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/transformer_api/#inference_batcher","text":"Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer). [source]","title":"inference_batcher"},{"location":"generated/model-serving/transformer_api/#resources","text":"Resource configuration for the deployment component (i.e., predictor or transformer). [source]","title":"resources"},{"location":"generated/model-serving/transformer_api/#script_file","text":"Script file ran by the deployment component (i.e., predictor or transformer).","title":"script_file"},{"location":"generated/model-serving/transformer_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/transformer_api/#describe","text":"Transformer . describe () Print a description of the transformer [source]","title":"describe"},{"location":"generated/model-serving/transformer_api/#to_dict","text":"Transformer . to_dict () To be implemented by the component type","title":"to_dict"}]}